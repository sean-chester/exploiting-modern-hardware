# CSC 485C/586C Candidate Project Ideas

_A preliminary list of possible project ideas. To be expanded with more ideas and more details, especially on any topics that are selected._


## Defining Characteristics of a Suitable Problem

 1. Should not be excessively difficult: remember that this is a term project, not a thesis. You want a problem for which you can come up with pretty good solutions in a matter of weeks, and whose complexity isn't so bad that you cannot run an implementation on the order of minutes nor scale to at least tens of thousands of points.
 2. Should not be excessively easy: embarrasingly parallel problems are those for which the naive algorithm is already data-parallel and close to optimal in terms of overall work.
 3. Related to point (2), should require the comparison of data points to each other in order to calculate the solution. This typically leads to naive run-time complexities that are quadratic or worse and state-of-the-art sequential run-time complexities that are linear or worse.
 4. Should have a well-defined, deterministic mapping between input values and output (more specifically, represent a surjective function).
 5. Be exciting and fun!


## Problems Grouped by Theme

The 12 suggested problems below are grouped thematically. The problems are non-exhaustive and the themes are non-exhaustive.


### Graph-data processing (e.g., social network analysis)

These problems are ideal for somebody with a particular interest in either graph algorithms or working with social data.


#### Calculating the PageRank value of every node of a graph

**Difficulty**: 4

[PageRank](https://en.wikipedia.org/wiki/PageRank) was the original idea that differentiated the Google search engine. It still forms a major component of web search rankings. The basic principle is to determine, for each webpage, the probability that a random walk over web links will arrive at that webpage. The principle extends to any sort of graph and has also been used to evaluate the "importance" of a person in a social network.

At web-scale, this is typically solved with distributed computing, but in this project you would take a smaller graph (such as a modest-sized social network of a few hundred thousand people) and use your optimisation and shared-memory parallelism skills to determine the PageRank of every node.

#### All-pairs Shortest Paths

**Difficulty**: 3

The [All-Pairs Shortest Paths](https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm) problem takes as input a graph and calculates a |V|x|V| matrix in which each cell (i,j) gives the shortest path between the i'th and j'th nodes of the network. It is instrumental in analysing some graph properties, such as the diameter of the graph, that help gain analytical insight into the nature of a network.

For this problem, there are already rather well-known algorithms (such as the linked one above). Most likely, you will be trying to produce quality implementations of them on several architectures, rather than generating novel ideas.


### Machine Learning Primitives and Natural Language Processing

These problems represent unsupervised machine learning techniques that are relatively simple conceptually. They are ideal for somebody who has a primary interest in machine learning (especially natural language processing) and wants to apply parallel computing to non-trivial cases thereof.

#### Brown Clustering

**Difficulty**: 5

[Brown clustering](https://en.wikipedia.org/wiki/Brown_clustering) is a hierarchical clustering algorithm to produce numeric representations of words. In comparison to simply giving ids to words, it captures semantic and syntactic meaning from context through the use of average mutual information. A [mulitcore c++ implementation](https://github.com/percyliang/brown-cluster) has been widely used in the field of natural language processing since 2005, but re-implementing it will be very difficult considering the code quality. No GPU implementation is known to exist.

#### Simplified Word Sense Disambiguation (WSD)

**Difficulty**: 3

As you _check_ this project description, you know I'm not suggesting to _slow it down_. The word "check" is polysemous; i.e., it has multiple meanings. Determining which meaning of the word is meant is quite easy for humans but a challenging Natural Language Processing (NLP) task for machines. The WSD problem is to determine, for each word in an input text, what definition is meant.

While there are many complex semi-supervised machine learning algorithms to solve this problem more effectively, this project focuses on [a simpler algorithm](http://ezproxy.library.uvic.ca/login?url=https://dl.acm.org/doi/10.1145/318723.318728) by Lesk (1986). Here, the sense of a word is determined by the number of other words in the surrounding sentence that occur in its definition. The algorithm is heavily memory-bound and thus a good candidate for the types of optimisations and parallelisation approaches studied in this course.

There is also a simplified version of this algorithm. See [Wikipedia](https://en.wikipedia.org/wiki/Lesk_algorithm#Overview) for an example.

Note that this is not a very effective algorithm for WSD, but will give some idea about the problem while also providing a great context to study the concepts in this course.


#### Singular Vector Decomposition (SVD)

**Difficulty**: 3

[SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) is a matrix factorisation problem that relates closely to spectral decomposition (a state-of-the-art approach to clustering) and is presumed to be the result of the hidden layer in the deep neural network to produce word embeddings like word2vec. Given its long standing in both the linear algebra and machine learning communities, it is a densely studied topic, but one that provides a very strong foundation for modern machine learning techniques.


### Data Mining

Data mining is related to unsupervised machine learning, but is more algorithmic than algebraic; it is the discovery of new knowledge, according to a prescribed notion of "interestingness".

#### Density-based clustering (DBScan)

**Difficulty**: 5

[DBScan](https://en.wikipedia.org/wiki/DBSCAN) is an algorithm for detecting clusters of Euclidean points based on local variations in density. In comparison to previous clustering algorithms, such as k-means, it particularly excels at discovering clusters with irregular shape. The main challenge parallelising the algorithm is that the problem is ill-defined: a "correct" solution is one that is the same as the output of the original "algorithm." GPU implementations exist in literature, but they may offer significant room for improvement.

#### Subspace outlier detection

**Difficulty**: 5

Outlier detection (also known as anomaly detection) is the identification of data points that don't fit the underlying patterns of the dataset. In a Euclidean sense, this can often be as simple as determining points that are very far away from all the other clusters of points. However, such a notion is compromised by the Curse of Dimensionality; when working with high-dimensional data, the Euclidean distance between any two points degenerates to an indistinguishable value from any other two points, and outlier detection methods often return spurious results.

Thus, the area of [subspace outlier detection](https://link.springer.com/chapter/10.1007%2F978-3-642-01307-2_86) tries to identify algebraic subspaces in which outliers can be more reliably identified. In this work, you would try to parallelise these ideas.


### Data Summarisation

Often an analyst or a Data Scientist is given a new dataset to investigate and it is hard to even know where to begin. Scale and dimensionality exacerbate this problem. These methods are unsupervised approaches to generating a subset of more interesting data points from a large dataset that could pose useful starting points for a more thorough follow-up. These problems could be ideal for those who envision a career in data wrangling.

#### The skyline problem (pareto frontier)

**Difficulty**: 3

[Pareto optimality](https://en.wikipedia.org/wiki/Pareto_efficiency) is a concept that originated in Economics to describe a game theoretic strategy that *dominates* all alternatives; i.e., no matter the circumstances, it is the best choice. Many data analysis tasks can also be thought of as game theoretic; e.g., selecting a product from an online shop/aggregator (such as Amazon or Skyscanner) is a game theoretic problem of determining the best trade-off of quality and price relative to an adversary (the retailer or service provider) who is also involved in the transaction but has the opposing pay-off. [The skyline problem](http://delab.csd.auth.gr/papers/IISA2015tpm.pdf) is to select all the points from a dataset that offer the maximisation of *some* trade-off. Depending on the amount of correlation in the data, it can reduce reduce a dataset to just a few points or only by half.

I have done extensive research on this problem and published source code online. This dramatically reduces the difficulty of a problem that otherwise would be very difficult. However, it could be a great choice for a group that wants a lot of support and would rather focus on techniques, concepts, and applications than trying to come up with novel solutions to a problem.

#### k-Regret query

**Difficulty**: 5

The primary problem with the skyline operator above is that the result size is determined by the dataset characteristics, not by the user. [The k-regret query](https://www.cc.gatech.edu/home/jx/reprints/Nanongkai%20et%20al.%20-%202010%20-%20Regret-minimizing%20Representative%20Databases.pdf) addresses this with a different economic concept, that of measuring "regret"; this refers to the extent by which a customer is disappointed by receiving a secondary choice instead of the first one. Applied in the context of data summarisation, the objective is to find a fixed-size subset of data points such that an analyst in the worst-case is as close as possible to the original data; i.e., the "regret" of summarising the data is minimised.

To my knowledge, this has not been studied in a parallel context.


### Event log processing

A common trend emerging in companies that deal with a high volume of data is to move towards a "log model" in which the _ground truth_ of the dataset is an append-only log of events that occur. For a company like LinkedIn, this could include a user joining a company, quitting that company, joining another company, then rejoining the original company again. The current state of the world is understood to be the result of applying all events in the order in which they appear in the log. Having this log-driven approach simplifies a lot of transactional processing, but creates challenges for analytical processing (e.g., Data Science).

#### Log Compaction

**Difficulty**: 3

An interesting [LinkedIn blog post](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying) illustrates the problem of "Log compaction", also known as _deduplication_. Given a (very long) sequence of events, the task is to produce a new log that consists only of the most recent values for every unique entity in the dataset.

This problem appears simple on the surface, but like our LUV problem in class may have subtleties that arise when trying to optimise it. The problem gives particular insight into the types of problems that social network companies like LinkedIn may need to solve.

#### Time Series Construction from Logs

**Difficulty**: 4

An [Uber blog post](https://eng.uber.com/aresdb/) describes their new GPU-powered analytics engine for constructing real-time time series dashboards for analysts. It also takes as input a log of events. The objective, given some arbitrary selection predicates, is to very efficiently construct a histogram (i.e., "time series") of how a given metric has been varying over the past, say, 24 hours.

While Uber open sources their code, the challenge here would be in producing a solution that is more general than what they can create given their company-specific business cases. They also use Thrust instead of CUDA. This would be an ideal project for a student looking to really do an industry-relevant project, knowing that it may be difficult to match Uber's published performance numbers.


### Complex Data Joins

In a basic databases course like CSC 370, you would have learned based SQL syntax for expressing the join of two tables based on a shared join key. Some joins are very straight-forward, where you simply match the values (an _equi-join_), but others can be expressed with more complex predicates, such as an inequality operator like <. These problems push that complexity to the extreme by trying to join on non-numeric types and are ideal for a student interested in the challenges of integrating insights from multiple data sets.

#### Exact substring matching

**Difficulty**: 4

This problem featured in [a programming contest at a conference in 2012](http://memocode.irisa.fr/2012/) and has applications in, for example, Computational Biology. The task is to identify partial string matches in one dataset, given an input string. In a complete example, one has two lists of strings and is trying to identify all partial string matches of any string in one list to any string in the other list.

#### Optimisation of Nussinov's (1978) Algorithm for RNA Folding

**Difficulty**: 5

RNA folding algorithms determine the best matchings of nucleotide base pairs in an input sequence. The algorithm of [Nussinov et al. (1978)](https://doi.org/10.1137%2F0135006) seminally introduced the use of dynamic programming (DP) via a clever mathematical induction on the energy requirements of a pairing. It relied on some simplified assumptions about how matched base pairs could nest. Since then, many subsequent folding algorithms adopt a similar DP approach to broader classes of pairings that more closely reflect biological reality.

Optimising and parallelising the Nussinov Algorithm is thus a great step towards better understanding how to optimise more advanced matching algorithms. Indeed, improving Nussinov's Algorithm is still of great interest to the Computational Biology field, as evidenced by recent top-venue publications:

[The use of tiling, as we've studied in January](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2785-6)

[The use of multi-core and GPU parallelisation](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-S8-S1)

[See here](http://rna.informatik.uni-freiburg.de/Teaching/index.jsp?toolName=Nussinov) for a visual introduction to Nussinov's Algorithm. You are not expected to match the results of the BMC paper above, but they will provide a strong benchmark as to how close the results you derive in this course project approach a top-level research paper in the field.

#### Temporal interval join

**Difficulty**: 5

Another very common type of data (e.g., in the log files of the previous theme) is temporal. One can construct time ranges, i.e., _intervals_. Given two different logs, e.g., produced by two different sensors or tracking two different types of information, the task is to find all entity pairs who have an overlapping time interval in each log.

This problem has been [recently studied](http://www.vldb.org/pvldb/vol10/p1346-bouros.pdf) on multi-core machines at a top database conference, but the provided solution can probably be improved quite drastically. 
