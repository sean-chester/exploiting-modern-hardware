# CSC 485C/586C Term Project: Interim Report 1

*An application of the single-core course concepts to a concrete data management problem of your choice*


## Objective

In the first interim report, you should apply pre-parallel concepts that we have studied in the course. These will naturally help your parallel performance later, but also establish a reasonable single-threaded baseline. It also serves as an opportunity to clarify your project objectives.

You should submit a report describing the problem that you are studying, as well as your attempt to optimise a single-threaded implementation thereof.


## Submission

Each group should try to apply some concepts from the course so far in order to obtain a >5x speed-up over a baseline (probably unoptimised) implementation. You are advised to take memory layout and data access patterns into account. You should submit a report based on your findings that adheres to the evaluation rubric below.

The report should aim for roughly ten double-column pages, excluding front matter and back matter (e.g., reference lists, non-critical appendices). Five pages is too few and twenty-one pages is too many. Length does not correlate strongly with grade.

Additionally, any resources needed to recreate the experiment results should be provided, e.g., by a time-sensitive link to a cloneable `git` repository.


## Evaluation

This interim report is evaluated according to eight criteria. Each criterion is marked on a 3-point scale: {2 = Fulfilled; 1 = Partially fulfilled; 0 = Unfulfilled}. The evaluation will be _strict_ with _extensive_ formative feedback, and you will have the opportunity to improve the grade for this interim report by addressing the feedback on subsequent submissions. That said, it is highly probable that some groups will obtain an 8/8 score.

The criteria are listed below, along with their evidence of fulfillment. It is advisable to have a section in your report corresponding to each criterion, as this will make your contributions more evident.

### Introduction and Context

*In a typical data management research paper, the introduction describes the objectives of the research and its importance.*

A good contextualisation of your research will:
  - describe clearly what is the problem that you are investigating, at a level that any computer scientist could understand
  - describe the importance of the problem, e.g., its impact on society
  - motivate why it is important to be able to solve this problem faster

The difference between a 2 and 1 on this criterion will be the clarity and compellingness with which these three elements are expressed. As a tip, a figure can help describe a problem immensely.

### Algorithmic Description

*In a typical data management research paper, the methods section describes a novel algorithm or data structure*

In this project, you are not expected to innovate a new algorithm, but to optimise the implementation of an existing one. You should describe the algorithm (or function or kernel) that you are optimising. An effective description of that algorithm will:
  - make the algorithmic control flow clear
  - describe input and output formats (i.e., data structures) explicit
  - clearly describe why the algorithm is very efficient, maybe the best known
  - relate to the higher-level description in the introduction

The difference between a 2 and 1 on this criterion will be how clearly you illustrate the algorithm. As a tip, pseudocode can be really helpful, especially if it's limited to <15 lines.

### Experiment Setup

*As we learned in class, an experiment typically consists of a benchmark on a test set (often random data), evaluated in a way to minimise the effect of randomness*

In your report, you will describe a good choice of:
  - dataset: it needs to be large enough to stress your system and distributed in a manner that is reflective of reality
  - queries: (if relevant) your queries should be well distributed and representative of real workloads
  - setup: you should have an appropriate benchmark in place that you can make claims that are convincing enough that a reader couldn't claim you hand-picked the best experiment results

The different between a 2 and 1 on this criterion will be the fairness and impartiality in your experiment design. As a tip, wrap your experiments in a well-selected set of distinct runs, as in our benchmarking lecture.

### Overall Comparison

*In a typical data management research paper, the improvements are reported in terms of speed-up in overall performance*

In your report, you should demonstrate a clear comparison between the performance before you apply your optimisations and afterwards. Probably, this information will be reported in terms of a table or a plot. As a tip, if you can vary input parameters, such as the size of the dataset, you will be able to make the case of scalability more clearly.

The difference between a 2 and a 1 on this criterion will be the clarity with which performance trends are presented, with respect to a well-chosen input parameter.

### Diagnostics

*We have learned in this class how to isolate the performance bottlenecks of a program. Here you demonstrate that on your problem.*

In your report, you should indicate:
  - what is the main bottleneck for performance in your program, prior to the optimisations that you have introduced
  - evidence to demonstrate that you have correctly identified the bottleneck

The difference between a 2 and a 1 on this criterion is how convincing is your evidence that you have correctly identified the bottleneck.

### Single-threaded Optimisation

*The point of all this is to make things faster and more scalable (less resources, more data, more speed, more interactive, etc.)*

In your report, you should indicate:
  - clearly, what optimisations you have made to the original data structures and/or algorithm
  - a performance speed-up as a result of your optimisations

The difference between a 2 and a 1 on this criterion is whether you achieve a >5x speed-up with your optimisations and whether they are clearly explained. As a tip, you should probably try to apply the techniques that we have learned in the class so far, as your projects (if you have discussed them with me) have been pre-screened to determine whether those techniques are plausible.

### Evidence to support optimisation claims

*The main argument in a research paper (any area) is evidence that the suggested improvement really has an impact*

In your report, you should provide some plots that show:
  - your optimisations have impacted overall performance
  - the cause of that performance improvement is directly a result of the optimisations that you made, for the reasons you claimed

The difference between a 2 and a 1 on this criterion is whether the evidence clearly draws a link between the diagnostics, the overall performance, and the single-threaded optimisation.

### Presentation Clarity

*No report is effective if the arguments are not presented clearly*

For clarity, you should ensure:
  - the text is easy to read and comprehend
  - figures are aesthetically pleasing and informative
  - the report is well laid out and well structured

The difference between a 2 and a 1 on this criterion is how easy the report is to mark. It's more subjective than the prior criteria.

## Postscript

Please note that the following resources will be helpful, but have not yet been released:
  * A wrapper for the PAPI library to write in-software performance profiling
    + but you could still try locally to install the PAPI library: https://icl.utk.edu/papi/
    + you may also be able to use _perf_, depending on how long-running your kernel is. I recommend trying both `perf stat` and `perf stat -d` independently. See [this reference](http://www.brendangregg.com/perf.html) for help.
  * More problems that you may find interested (e.g., other text processing problems that aren't quite as challenging as Brown Clustering)
  * A sample solution for the first interim report, using the LUV problem that we've studied in class.
