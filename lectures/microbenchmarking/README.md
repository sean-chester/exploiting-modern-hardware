# 04 - Microbenchmarking Lecture (Live Coding)

This lecture introduces the notion of microbenchmarking, which we will use extensively to compare ideas in the most critical of indicators: raw run-time performance. In the previous lecture, we designed several algorithms for solving the Lone Unique Value (LUV) problem. The asymptotic arguments in favour of each are relatively straight-forward. Nonetheless, today, we experimentally evaluate them. Our objectives are:
 * to think about challenges in benchmarking data (e.g., generating appropriate test data)
 * to learn how to generically define a benchmark library that we can reuse for other problems
 * SPOILER: to challenge the random access model (RAM) of computation that we typically use to compare algorithms

## Microbenchmarking

Microbenchmarking is a data-driven approach to comparing ideas. The comparison can be on ideas as simple as selecting a container (e.g., hash map versus array) or as complex as comparing algorithms. The _micro_ prefix implies that the overall size of the function, in terms of the number of instructions, is small. As the LUV problem is relatively simple, our implementations are small and we can microbenchmark them.

The challenges with microbenchmarking are entirely within creating representative cases. If the test data is too small, we may not stress the system in the same was as a live release would; if the test data is too large, the time to run the microbenchmark may be prohibitive and discourage testing. If the data distribution is non-representative, we may see performance patterns that are similarly non-representative. Thus, in class, our focus was on designing a "good" representative random vector that adhered to our problem specifications. This is discussed in the inline comments in benchmarking.cpp

## Results

Perhaps surprisingly, the asymptotic analysis did not predict run-time performance accurately. If you re-run the benchmark, you should find that the O(n lg n) sort-based algorithm outperformed the O(n) map-based algorithm. This illustrates very effectively why we should benchmark our ideas, even after a theoretical analysis.

Possible explanations include:
  * the input size was too small. We can plot our microbenchmark for varying input sizes and analyse the trends to confirm this. However, if our input size is representative of an expected workload in production, then this isn't a very satisfying explanation. Recall that the asymptotic analysis applies in the limit: i.e., for a potentially extreme (non-realistic) input size.
  * there are errors in either our benchmark or algorithm implementations. This should be the first assumption in normal circumstances.
  * the coefficients hidden by the asymptotic analysis hide real performance. This requires further explanation: why would the constants be so much higher for one algorithm than another?
  * the theoretical model (in this case, RAM) used in the analysis is inappropriate for the problem at hand. Spoiler: this will very likely be the case when we move the GPUs.

What we don't know yet is how to investigate further. If the sort-based algorithm isn't fast enough, and the bit-based algorithm isn't correct enough, most people won't know what to do. They'll have to hire a consultant with the expertise that we're about to build.

## Coming Lectures

  * We'll go one level of abstraction deeper, considering not just algorithms and implementations, but also hardware
  * We'll learn how to isolate the performance bottlenecks of each of these implementations (using the Intel optimisation guide) to determine if the limitations are algorithmic or implementation-specific, compute-bound or memory-bound
  * We'll develop a much deeper insight into architecture-conscious algorithmics, to see how we could optimise these algorithms for cache or predictability
