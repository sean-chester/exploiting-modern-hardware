# 04 - Benchmarking Lecture (Live Coding)

This lecture introduces the notion of benchmarking, which we will use extensively to compare ideas in the most critical of indicators: raw run-time performance and scalability. In the previous lecture, we designed several algorithms for solving the Lone Unique Value (LUV) problem. The asymptotic arguments in favour of each are relatively straight-forward. Nonetheless, today, we experimentally evaluate them. Our objectives are:
 * to think about challenges in generating appropriate test data for benchmarking
 * to learn how to generically define a benchmark library that we can reuse for other problems
 * SPOILER: to challenge the random access model (RAM) of computation that we typically use to compare algorithms without considering its inherent assumptions

## Benchmarking

Benchmarking is a data-driven approach to comparing ideas. The comparison can be on ideas as simple as selecting a container (e.g., hash map versus array) or as complex as comparing algorithms. A benchmark nests an idea inside a loop that runs many times (to balance randomness) and uses timing libraries to calculate how long the loop takes. If we benchmark two ideas, we can then compare their average run times.

There are two primary challenges with benchmarking:
  * Identifying representative and feasible test data
    - If the test data is too small, we may not stress the system in the same was as a live release would
    - If the test data is too large, the time to run the microbenchmark may be prohibitive
    - If the data distribution is non-representative, we may see performance patterns that are similarly non-representative.
  * Ensuring that the benchmark times what we are trying to time
    - We need to minimise overhead in running the loop so that the time truly reflects the cost of the target function
    - We need to add enough overhead that the compiler does not optimise away (discard) any of the function calls
    - We need to be mindful of hardware effects like cache locality _in the benchmark code_ which could favour one idea over another 

Thus, in class, our focus was on designing a "good" representative random vector that adhered to our problem specifications. This is discussed in the inline comments in benchmarking.cpp. We also added small overhead to accumulate the results of our repeated function calls; we print the sum at the end so that the compiler does not optimise away our function calls. For more examples of compiler optimisation interfering with benchmarking, see: https://stackoverflow.com/a/2842707/2769271

## Results

Perhaps surprisingly, the asymptotic analysis did not predict run-time performance accurately. If you re-run the benchmark, you should find that the O(n lg n) sort-based algorithm outperformed the O(n) map-based algorithm. This illustrates very effectively why we should benchmark our ideas, even after a theoretical analysis.

Possible explanations include:
  * the input size was too small for the lg n term to be visible. We can plot our microbenchmark for varying input sizes and analyse the trends to confirm this. However, if our input size is representative of an expected workload in production, then this isn't a very satisfying explanation. Recall that the asymptotic analysis applies in the limit: i.e., for a potentially extreme (non-realistic) input size. 
  * there are errors in either our benchmark or algorithm implementations. This should be the first assumption in normal circumstances.
  * the coefficients hidden by the asymptotic analysis hide real performance. This requires further explanation: why would the constants be so much higher for one algorithm than another?
  * the theoretical model (in this case, RAM) used in the analysis is inappropriate for the problem at hand. Spoiler: this will very likely be the case when we study GPUs.

What we don't know yet is how to investigate further. If the sort-based algorithm isn't fast enough, and the bit-based algorithm isn't correct enough, what should we do? Also, how do we explain the mismatch between theory and practice here?

## Coming Lectures

  * We'll go one level of abstraction deeper, considering not just algorithms and implementations, but also systems/hardware
  * We'll learn how to isolate the performance bottlenecks of each of these implementations (using the Intel optimisation guide) to determine if the limitations are algorithmic or implementation-specific, compute-bound or memory-bound
  * We'll develop a much deeper insight into architecture-conscious algorithmics, to see how we could optimise algorithms for cache or predictability
