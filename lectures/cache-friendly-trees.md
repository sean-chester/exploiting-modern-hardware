# Making pointer-based structures more cache-friendly

 * preliminary announcements
   + midterm in 2.5 weeks, last day before reading break
     - will start disseminating some info and foreshadowing materials to help you focus your preparations
     - material cut-off end of next week
   + today or tomorrow will post submission for first interim project report
     - later than expected, so submission deadline will be delayed until after reading break
     - should have picked a project by now and started thinking about which techniques from lectures might apply

 * Quick review + extend last class (first Drepper experiment): cache-friendly linked lists
   + contiguous allocation, improves spatial locality
     - can be done for nearly any type of data structure, even trees and graphs (and other pointer-based structures)
     - potentially: allocate pool of nodes contiguously rather than as needed
     - potentially: "defragment" data structures on-the-fly by periodically reallocating space contiguously
     - why we typically prefer dynamic arrays, e.g., std::vector<>, to non-contiguous allocation, e.g., std::deque<>
     - one of the first opportunities for data structure optimisation
   + layout according to predicted access pattern
     - sequential layout corresponds to ordering them in same pattern they will be accessed
     - question to class: what unique property of a linked list makes this easier than other pointer-based structures?
       * single successor, i.e., can create a clear access order (a "total ordering" rather than a "partial ordering")
     - to achieve: re-sort nodes (and update links) in order of retrieval

 * paper has 3 techniques
   + compression (we've discussed this extensively), e.g.,
     - reordering fields in a struct
     - splitting hot and cold data to compress *working set size*
   + colouring, i.e., for set-associativity (discussed at end of lecture, cut if no time)
     - how to map a cache address to a virtual/physical address in memory?
       * direct access = one-to-many mapping
       * set-associative = each address maps to a set of 4- or 8- possible locations in cache
       * we'll likely talk about it more for gpus ("bank conflicts") but not now
   + clustering, i.e., reorganising data structures to improve locality, focus today + yesterday + tomorrow

  * How to extend linked-list (single successor) to more general pointer-based structures (e.g., a binary tree)?
    + can still allocate all the data contiguously, but in what order?
    + could use BFS or DFS or in-order, etc.
      - will work excellently if it matches the order we will traverse the data!
      - that's the difference we can make as a designer: to design it according to the anticipated usage pattern
    + what if not traversing entire tree, but just one path to one leaf?
      - c.f., "cache oblivious" blog post in reading list for figures
      - for large tree (i.e., not fully cache-resident), breadth-first order will guarantee a cache miss at every level of tree
      - for large tree, depth-first order will be inconsistent
        * traversals to leftmost leaf will incur minimum possible cache misses
        * traversals to rightmost leaf will guarantee a cache miss at every level again
        * worst-case isn't better, but average case is
      - pattern that recursively splits tree in half gives consistently good pattern
        * spatial locality because half of nodes are immediately succeeded by their children
        * temporal locality because most commonly accessed nodes (top of tree) share cache line, so unlikely to evict each other
      - however, if know statistics of usage patterns, can optimise towards those for even better average case behaviour

  * How to extend to graph (which cannot be easily split recursively in half)?
    + temporal locality: order by graph properties like PageRank that will sort "hot" data together to front of contiguous allocation
    + spatial locality: to extent possible, store such that each vertex is succeeded by its immediate neighbours
      - can be challenging!

  * Tomorrow, we will see the application in research (frequent itemset data mining problem) of these layout considerations
