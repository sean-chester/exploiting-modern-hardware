# GPGPU and the SIMT Computation Model

Start page: https://www.nvidia.com/en-gb/data-center/pascal-gpu-architecture/

## Background

  * Who in class has programmed GPUs? With CUDA? (circa 4)
  * What is a GPU?
    + graphics card
    + custom-designed for massively parallel FLOPs to compute pixels of graphics
    + Early 2000's, hacked to general purpose parallelism by manipulating input/output
    + Been around for a couple decades. Why is it considered modern hardware?
      - 2008, NVIDIA announced their _Compute Unified Device Architecture_
        * Tools to do _compute_ on the same GPU device that is mean for graphics; i.e., first time hardware was _meant_ for general compute: called GPGPU: "general purpose graphic processing units"
    + Now it powers big matrix operations for AI, where the money is at; but still can do gpgpu.

  * Reason it's so parallel, recall back to Moore's Law, heat dissipation
    + Recall Figure 1, http://www.gotw.ca/publications/concurrency-ddj.htm, power drop-off
    + Recall "Charting the landscape," https://herbsutter.com/welcome-to-the-jungle/
    + Weak core versus fat core
      - We've learned about all the fat core techniques for hiding memory latency (branch prediction, deep cache, OOE)
      - GPU gives low freq cores, e.g., 1.1GHz, but lots of them

## The SIMT Computational Model

  * Recall SIMD, finish yesterclass's example: forgot to simd'ise inner loop
    + Comparison: why multicore in outer loop and simd in inner loop?
    + What is different about SIMD compared to multicore? (**single instruction**)
  * GPU follows *SIMT* model, similar to SIMD
    + Figure from: https://slideplayer.com/slide/10622104/, slide #35
    + 32 threads that are "steplocked" together; must always do the same thing
      - All 32 threads share the same front-end, i.e., must have same single instruction
      - At branch, if threads in same warp diverge, execution is serialised: first the if executes, then the else
      - If no else, then throughput still suffers as some threads momentarily idle
      - Thread saturation = keeping all threads active, major challenge of GPU algorithm design
    + Typically map each thread t_i to each data element e_i, with data laid out in long array
      - New type of "locality": want conditional instructions near each other to behave the same = avoiding branch divergence
  * Hides latency with _context switching_
    + Good design = launch orders of magnitude more threads than physical cores
    + While one warp awaits memory accesses, it is swapped off the hardware and and a "ready" warp is swapped in
    + Like hyper-threading, except:
      - Not round robin context switching, but in order of who is ready
      - Much larger scale (100's, not 2)
      - Much cheaper context switch

## The GPU Memory Model
   
   * Shown here: https://www.ce.jhu.edu/dalrymple/classes/602/Class13.pdf (memory layout figure, missing pcie)
   * GPU (a.k.a., "device") and CPU (a.k.a. "host") have separate memory, must copy/transfer between over high bandwidth, high latency PCIe3
     + Programs should avoid repeated and especially random transfers betweeh host and device memory
   * Global memory = shared among all warps
   * Shared memory = cache local to thread block; scratch memory for working computations
   * Also texture cache is faster than shared memory, because read-only. No need for mechanisms of cache coherency, etc.
   * Must manage memory explicitly, no STL in CUDA!
