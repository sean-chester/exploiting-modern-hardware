# Intra-cycle Parallelism

_Review of BitWeaving paper (SIGMOD 2013)_

 * Announcements:
   + Battlesnakes intro by guest at beginning of class next Friday
   + First interim report guidelines posted to conneX
   + Goal today is to stretch your imagination a bit
     - think of unsigned ints as collections of bits and collections of bits as treatable like ints

 * Today, transition towards parallelism, now that we have the skills to combat the memory wall
   + Still no threading yet though: rich source of parallelism available
   + Question: what is "parallelism"?
     - Doing two things (probably compute operations) at once
   + Standard forms of parallelism ("XX-level parallelism")
     - cloud / distributed
       * example of "shared nothing" in contrast to the "shared memory" models we'll look at in this class
       * the cores we want to use are connected via network, i.e., extra layer in memory hierarchy, more latency
     - multicore
       * the use of more than one physical processor (a.k.a "core") to solve a problem
     - hyperthreading
       * the existence of multiple hardware threads that share one core and alternate access to the same registers/front-end every cycle
       * even L1 cache is shared between hyperthreads
       * also called "simultaneous multithreading" (SMT) in order literature
     - deadlock
       * a case to avoid with parallel programs that indicates a lack of independence
       * neither of two threads can finish their work and free their resources until they obtain the resource that the other thread has locked
     - process-level parallelism / concurrency
       * concurrency is the _concurrent_ execution of several tasks/applications
       * parallelism is a special case of concurrency in which we achieve concurrency by assigning multiple concurrent tasks to duplicated physical resources, e.g., extra cores or extra instruction dispatch ports
     - task-level parallelism: having high-level tasks that don't depend on each other; at once, e.g., by different threads
     - instruction-level parallelism: having instructions that don't depend on each other, e.g., don't use the same variables
       * necessary for super-scalar cores, e.g., CPI < 1
     - data-level parallelism: having multiple data elements to which the same instruction should be applied
       * this is what we usually think of and usually try to expose first
     - fine-grained vs coarse-grained: size of each parallel unit, e.g., do we apply one instruction or an entire method to each data element?
     - can apply the terminology to anything, about how we think of "independence"

 * Recall our discussion of SOA vs AOS and splitting hot/cold data
   + Question: assuming unsigned ints, how would you take that to the extreme?
     - Figure 5: split by bit
     - How does this help? How can we use individual bits?
       * most significant bit (MSB) helps with many basic predicates
         + inequalities start at MSB
         + equality can actually use all bits in any order
         + possible to fully answer a predicate on a large amount of data by only looking at a small fraction of the bits
       * now in a 32- or 64-bit word/register, we can actually process 32- or 64- numbers at a time
         + speed-up depends on how many of the bits we need to resolve the query

     - Less extreme: horizontal packing, figure 3. Often domain doesn't go to 2^32 or 2^64.
       * this is actually SIMD: data-level parallelism, but done in software/data structure design rather than in hardware
       * contrast to Jacobs packing, non-uniform types. advantages? disadvantages?
         + changing order of jacobs packed struct for purpose of sorting
           - swap country and age field, then sorting by these 128-bit numbers achieves a two-level sort!
           - i.e., alternately seeing it as one 32-bit or 64-bit word and also as a collection of small packed fields
