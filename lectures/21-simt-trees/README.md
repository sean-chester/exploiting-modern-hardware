# SIMT-Friendly Spatial Grid/Tree Partitioning

## Overview

Today is a synthesis lesson, where we put together many concepts from throughout the course so far. In particular, we are going to concurrently leverage four layers of parallelism (and enable a fifth) by combining together:

 * [Instruction-Level Parallelism (ILP)](../ilp/)
 * [Branch-free Code](../branch-pred/)
 * [Data Packing](../intra-cycle-parallelism.md)
 * [OpenMP Parallel-For Work-Sharing](../15-openmp/)
 * [Work-Efficient Algorithms](../17-work-efficiency/)
 * [SIMD Parallelism](../19-simd/)
 * [SIMT-Friendly Data Structure Design](../20-simt-graphs/)

Multi-dimensional data processing typically involves determining the spatial proximity of points such as which points are nearest each other. Thus, work-efficient algorithms nearly always impose some sort of data ordering that can group nearby in memory points that are nearby in space. [We already observed this](../17-work-efficiency/) when we ordered points by L1/Manhattan Norm so that we could break an inner loop earlier when trying to determine which points are "isolated."

Sorting multi-dimensional points for proximity is challenging because they inherently present a _partial_ rather than a _total_ order. In our earlier example, we projected the points into one dimension and sorted them by their projections. The other classic approaches are a hierarchical/tree-based grouping (e.g., an [R-tree](https://en.wikipedia.org/wiki/R-tree)) and [a static grid](https://en.wikipedia.org/wiki/Grid_(spatial_index)). The latter often leads to better parallel performance [2], partly because there are no memory loads involved in navigating a static grid. From a point's coordinates you can directly calculate its grid cell. For example, given the latitude and longitude of a location, you can immediately assert its [Marsden Square](https://en.wikipedia.org/wiki/Marsden_square) without knowing anything about any other points in the dataset: you simply divide the latitude and longitude by the appropriate grid size. Ergo, grids have a higher compute:memory ratio than trees, which is good for parallelism.

However, trees offer the advantage of pruning (or summarising) data earlier, at higher levels of the hierarchy. Today, we will build in parallel [an octree](https://en.wikipedia.org/wiki/Octree), which is a hybrid tree/grid. As a tree, it offers the typical hierarchical/recursive pruning advantages. However, the fixed `2^d` children of each node are defined statically by splitting every dimension in exactly half. This provides the advantages of a static grid in terms of computing in which child a point is found, just from its coordinates. Thereafter, it can be used by parallel (e.g., GPU) algorithms for work-efficient search space pruning.

The layout of the tree will be identical to our _flattened adjacency list_ from [the previous lecture](../20-simt-graphs/): all the points will be sorted in one large array (called "leaves") and an array of offsets (called the "directory") will indicate where each partition starts. The design _can be_ arbitrarily deep, but we will fix it to two levels today. Our task is to construct it using data packing, ILP, SIMD, _and_ multicore parallelism (following the ideas of [1]).

(To be very precise, there is a template class of data structures, where [a quadtree](https://en.wikipedia.org/wiki/Quadtree) is the 2d variant, the octree is the 3d variant, and I don't know what people call anything in 4 dimensions or higher!)

This task provides a second great example of how to think in a SIMT-friendly manner. In this live-coding lecture, we are given an unsorted list of points and will construct an octree from it using an algorithm that could be successfully ported to a GPU. It is our first example in this course of combining multiple layers of parallelism to achieve high parallel throughput.

## Problem Definition

Our task for this lesson is to **transform a set of 4d points into [a flattened octree](tree.hpp)**. Specifically, we would like to expose as many types of parallel as possible, and in a SIMT-friendly way.


## Code Instructions

There are three files in this lesson:
  
  * [tree.hpp](tree.hpp): a header file that defines the octree as a class and its member functions (e.g., building the directory level)
  * [build-tree.cpp](build-tree.cpp): an implementation of one highly-parallel algorithm to construct an octree 
  * [4d-points.hpp](4d-points.hpp): a static, 16B-aligned c-style array of 4d (floating point) points.

To compile the code, you will need the `-mavx` compile flag for SSE/AVX _and_ the `-fopenmp` linker flag to enable parallelisation with OpenMP:

> g++ -Wall -O3 -std=c++17 -fopenmp -mavx -march=native build-tree.cpp -o bt


## Conceptual Design

This lesson consists of a single implementation (no sequential baseline). Below is described how each design element is incorporated.

### Overall (SIMT-Friendly) Control Flow

The general algorithmic flow is mostly the same as for [building a flattened adjacency list](../20-simt-graphs/):

 1) For each data element (in this case, 4d point), in parallel and completely independently, calculate its leaf membership and metadata
 2) Sort all the data elements by partition id
 3) Construct the directory over top of the flat data array

The first two steps expose tremendously wide parallelism. The third step is much narrower, but also fully independent. In the previous lesson, the first two steps were reversed, but that's because we could compute the sort order without yet computing anything else (the sort was by `e.u`, which is given in the input).

In this lesson, the metadata that we compute is the grid cell, based on a decomposition of each dimension into 4 equal-sized slices, i.e., `4^4 = 256` grid cells. The grid cell has two components: the relationship to the midpoint (the first partitioning in an octree) and the relationship to the relevant quartile (the next deeper level in the octree). I.e., if a point's _x_-coordinate is larger than half the _x_-range, it is then compared to the upper quartile (3/4's of the range); otherwise, it is compared to the lower quartile. Each comparison produces a bitmask, with one bit per dimension; i.e., 4 bits per level of the tree.


### Bit-Packing

As indicated above, we need 4 bits per level of the tree to indicate which cell a point is in, i.e., we need 8 bits to differentiate `2^8 = 256` cells. We can pack the levels side-by-side into 1 byte: the first 4 bits corresponds to the top-level of the tree and the last 4 bits correspond to the second-level of the tree. If we then sort (step 2 of the algorithm) all of the points based on this 1 byte, they will effectively be sorted according to: a) the upper-level (most significant bits) and then b) the lower-level (least significant bits). This is the trick [we saw already](../intra-cycle-parallelism.md) for sorting 128-bit census data by age then country, just by treating the packed fields as one (unsigned) integer.

We can take this one step further by packing the point's id after the 1 byte that indicates the partition id. Then the sort will also retain the original input order, because the third (implicit) level sort key will be the point id/index in the original array. Even more importantly, it allows us to know _which_ points are in which partitions.

We can pack all this information into a regular 32-bit unsigned integer as long as the number of unique points consumes less than 3 bytes, i.e., `log( n ) < 24`. 


### SIMD Branch-Free Parallelism

Given that we are using points with 4 single-precision floating point dimensions, each point fits quite cleanly into a 128-bit SIMD register (4 lanes). As in [our SIMD lesson](../19-simd/), we can map the point-to-point computations onto intrinsics. It takes perusing the [Intel Intrinsics Guide](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#) in some detail to see that there are operations for lane-wise max, greater than comparisons, and bit logic. I recommend looking up in the Intrinsics Guide each of the functions used here to get a broader idea of what can be done with basic SIMD.

To make this work effectively, we need to make it _branch-free_. This is challenging because whether we compare a point to the first or third quartile depends on its relationship to the midpoint. We achieve this with some intricate bit manipulation. Note that the `_mm_cmp_ps()` instructions return -1 if true and 0 if false. A logical AND of two true values is -1 and anything else is 0. We logically OR together the comparison to _both_ quartiles, but use a logical AND (`_mm_and_ps()`) to turn off all third quartile bits for which the midpoint mask bit is not set. We use a logical NAND (`_mm_andnot_ps()`) to turn _on_ all first quartile bits for with the midpoint mask bit is not set. This adds a couple (vector) instructions to each inner loop, but completely eliminates the branch. Branch-free code often involves bit manipulation like this.

Moreover, as each quartile calculation is independent, both the calculation of the comparison mask and the logical (N)AND with the midpoint mask, we expose substantial ILP here.

The final instruction (`_mm_movemask_ps()`) simply converts each -1 lane into a single set bet and each 0 lane into a single unset bit; i.e., it transforms the 128 bits into 4 bits. 

### Multicore Parallelism

The final step is often the multicore parallelism. Rather trivially, we can [work-share](../15-openmp/) all three steps across parallel threads. As long as there are fewer than 16 cores, even the third step exposes enough parallelism to saturate the machine.

### Profiling/Analysis

In this case, we don't have a baseline against which to compare. Therefore, we cannot establish work-efficiency. However, we can still examine raw performance numbers and parallel scalability. Running on a single core of the Azure instance:

> numactl --physcpubind=0 ./bt

We observe performance similar to:

```
answer: 86
time: 16499.8 us
```

Sixteen milliseconds to construct a tree with a quarter million points seems quite reasonable. Running it on all six cores, we observe performance similar to:

```
answer: 86
time: 2958.86 us
```

This represents a very impressive parallel speed-up of `16499.8 / 2958.86 = 5.58×`, which, moreover, doesn't include at all the improvements obtained from vectorisation. In the end, on 6 cores, **we have built a search tree in 3 milliseconds** for a quarter million 4d points, with a layout amenable to subsequent GPU processing.

### Final Notes

As always, we should question the generality of what we have observed. This time even more than ever, because there was no baseline comparison. If you want to push this lecture further, I suggest testing the following rough edges, as they will give you a deeper understanding of what we have done.

 * **Try changing the dimensionality**. Obviously 4d points were not chosen arbitrarily: they fit the 128-bit SIMD width perfectly and their grid cells consume only 8 bits in the packed leaf data structure. Try first changing the dimensionality to `d=8` and adapting the code to fit. Does the wider SIMD offset the greater storage space required in the leaves? How would you handle dimensionalities that don't fit perfectly into a given SIMD width?

 * **Try changing the depth of the tree**. With 250.000 points partitioned into 256 cells, we would expect on average about a thousand points per cell. This is not a very fine resolution of data partitioning. How would you increase the tree depth by one and how does that affect the amount of parallelism exposed? How would you manage a trillion points?

 * **Attach an algorithm onto the data structure**. Especially if your project involves multidimensional data, how would you adapt your algorithm to take advantage of this (or a similar) data structure? Of course [1] uses it heavily to accelerate skyline computation.

## References

[1] Bøgh et al. (2017) "Template Skycube Algorithms for Heterogeneous Parallelism on Multicore and GPU Architectures." _ACM International Conference on Management of Data (SIGMOD)_, 447–462. http://ezproxy.library.uvic.ca/login?url=https://dl.acm.org/doi/10.1145/3035918.3035962

[2] Šidlauskas et al. (2009) "Trees or grids?: Indexing moving objects in main memory." _ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (GIS)_, 236–245. http://ezproxy.library.uvic.ca/login?url=https://dl.acm.org/doi/10.1145/1653771.1653805
 