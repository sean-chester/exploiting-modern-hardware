# Parallel Prefix Sums and Advanced Reductions

## Overview

The _parallel prefix sum_ is a classic training example for learning SIMD/SIMT parallelism. Prefix sums are a surprisingly common algorithmic primitive: we saw it already [when building an adjacency list](../20-simt-graphs/build-graph.cpp).

It is used to train data-parallel thinking because it appears on the surface to be an inherently sequential problem. When does sequentially, it involves `n-1` additions. However, any naive parallel method seems to require `O(n^2)` additions. Seeing how to decompose this into `O(n)` work that can be effectively parallelised, especially with thousands of threads, requires thinking quite differently about the problem.

In this self-paced lab, you have given an efficient sequential implementation and asked to outperform it with a GPU solution that you code up in CUDA. You will go through a few steps to arrive at an excellent solution [1], with hints released every day or two. In the end, you will not only have learned how to solve parallel prefix sums and thinks in a more SIMD-oriented manner, but also have learned a couple new parallel primitives that can be applied to many similar parallel problems.

## Problem Definition

You are given an array of integers:

> `[a_0, a_1, ..., a_{n-1}]`.

You are to calculate the prefix sum, i.e.:

> `[0, a_0, a_0 + a_1, a_0 + a_1 + a_2, ..., âˆ‘a_i - a_{n-1}]`.

(_Note that sometimes the prefix sum starts from `a_0` rather than `0` and then the final term includes `a_{n-1}`._)

## Code Instructions

There are two files in this lesson:
  
  * [prefix-sum.cu](prefix-sum.cu): an implementation of prefix sum in CUDA. A sequential CPU method is provided. You are to create a version that can run on the GPU. 
  * [../19-simd/xvals.hpp](../19-simd/xvals.hpp): reusing a c-style 1d vector of random float values

The code is saved as a CUDA file; so, you will need to compile it with `nvcc` (likely on the Azure instance):

> nvcc -O3 -o ps prefix-sum.cu

(_Note however that you could compile with `g++` if there is no CUDA code by prefixing the input filename with the `-x cxx` flag._)

## Design Sequence

### Sequential Baselines

We already have _two_ efficient sequential implementations, which you can use to verify your understanding of the problem. An in-place algorithm was provided [in a previous lab](../20-simt-graphs/build-graph.cpp) and an out-of-place algorithm is given in namespace `seq::efficient::`. These have asymptotically-optimal `O(n)` time complexity; so serve as a good baseline of performance. Running the baseline with 100 random trials, `./ps 100`, on the Azure (Broadwell) instance gives results similar to:

```
answer: 4.98307e+06
time: 10.03 us
```

This is already very fast, but let's try to solve the problem on the GPU.

### Naive Parallelisation

The simplest approach to exposing wide parallelism would be to have each thread compute a separate prefix sum, i.e., to parallelise over the outer loop. This is demonstrated in namespace `gpu::naive::` and, for comparison, a CPU-parallel version is given in namespace `par::naive::`. If we run the GPU version with the profiler, we observe performance similar to:

```
==47033== NVPROF is profiling process 47033, command: ./ps 100
answer: 4.98307e+06
time: 5816.72 us
==47033== Profiling application: ./ps 100
==47033== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   98.53%  65.959ms       100  659.59us  657.98us  661.53us  void gpu::naive::naive_prefix<float>(float*, gpu::naive::naive_prefix<float> const *, unsigned long)
                    0.89%  592.99us       100  5.9290us  5.8880us  6.0800us  [CUDA memcpy HtoD]
                    0.59%  394.11us       100  3.9410us  3.8720us  4.3520us  [CUDA memcpy DtoH]
      API calls:   82.02%  371.62ms       200  1.8581ms  5.3990us  360.09ms  cudaMalloc
                   15.40%  69.769ms       200  348.85us  13.699us  712.87us  cudaMemcpy
                    2.03%  9.1865ms       200  45.932us  6.9990us  106.20us  cudaFree
                    0.28%  1.2646ms       100  12.646us  11.499us  28.399us  cudaLaunch
                    0.13%  608.68us         1  608.68us  608.68us  608.68us  cuDeviceTotalMem
                    0.12%  534.18us        94  5.6820us     100ns  393.48us  cuDeviceGetAttribute
                    0.01%  52.698us       300     175ns     100ns     600ns  cudaSetupArgument
                    0.01%  42.000us       100     420ns     300ns     800ns  cudaConfigureCall
                    0.00%  16.099us         1  16.099us  16.099us  16.099us  cuDeviceGetName
                    0.00%  2.1000us         3     700ns     100ns  1.5000us  cuDeviceGetCount
                    0.00%  1.2000us         2     600ns     300ns     900ns  cuDeviceGet
```

First of all, observe that the execution time **increased** by a factor of `5816.72 / 10.03 = 580Ã—`! ðŸ˜¬ That's the sort of speed-up, not slow-down, that we might hope for on a GPU!

Looking at the profile, we see that GPU memory management (e.g., `cudaMalloc()`, `cudaMemcpy()`, and `cudaFree()`) were certainly factors. But our kernel itself (`void gpu::naive::naive_prefix<float>(float*, gpu::naive::naive_prefix<float> const *, unsigned long)`) required an average of `659.59us`, i.e., a slow-down of about 66Ã—.

This comes down to two principal points: work-efficiency and workload balance. Observe that the naive parallel solution requires `0+1+2+...+n-1 = n(n-1)/2` additions, which is `n/2`-fold more than the sequential algorithm. This is, frankly, quite dreadful.

But another factor comes into play. In principle, with _n_ threads, we could hope for a parallel complexity closer to `n(n-1)/2n`, as each thread assumes an equal share of the work. However, in `gpu::naive::`, thread `t_0` won't do any additions, `t_1` will do one addition, and `t_{n-1}` will do `n-1` additions. So, in fact, our slowest thread will compute `O(n)`  work.

A naive parallel analysis divides the work by the number of parallel cores. The _critical path_ of parallel code refers to the longest sequence of instructions that must be executed by one thread. In general, the parallel performance we can hope for is limited by the length of this longest sequence, because we have to wait for the slowest thread. Thus, to improve the performance of this prefix sum, we need to find a way to compute the answer with each thread close to the same number of computations, i.e., decrease the length of this critical path.


## Remaining Task

 * See if you can distribute the computations, at least conceptually, in a more balanced manner in order to decrease the length of the critical path. If you are stuck, you could refer either to [1] or to the circuit digrams on [Wikipedia's Prefix Sum page](https://en.wikipedia.org/wiki/Prefix_sum).


## References

[1] Harris et al. (2007) "Chapter 39. Parallel Prefix Sum (Scan) with CUDA." In _GPU Gems 3_. https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda
 