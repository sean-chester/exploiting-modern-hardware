/**
 * Introduction to multi-core analytics.
 * Sub-topics include the use of the OpenMP threading library
 * and the **parallel reduction** technique.
 *
 * Applying these techniques to a problem we used in the Branch Prediction lecture, the
 * _Sum Over Threshold_ problem. Given an input vector `v = <v_i>` and a threshold `t`,
 * determine `sum_{v_i\in v, v_i >= t}`
 *
 * For example, given the input <2, 3, 1, 8, 3, 4> and threshold 4, the response is:
 * 8 + 4 = 12.
 */

#include <iostream> // std::cout, std::endl
#include <chrono>   // std::chrono
#include <numeric>  // std::accumulate()
#include <omp.h>    // for multi-core parallelism

#include <vector>

// statically-defined input data
#include "unsorted-data.hpp"


namespace { // anonymous

/**
 * Branchless functor to zero a value if and only if it is strictly less than a threshold.
 * The branchless approach balances our parallel workload, because all work tasks are identical.
 * A branching approach (aside from harming branch prediction) could lead to imbalanced workload,
 * as some threads may have much better branch prediction and load latencies than others.
 */
template < typename T >
    struct branchless_sum
    {
        T const threshold;

        T operator ()( T const value ) const
        {
            // Avoid branch by multiplying value by the result of the condition (0 or 1)
            return value * ( value >= threshold );
        }
    };

} // namespace anonymous


namespace race_condition {

/**
 * A naive (and incorrect) deployment of OpenMP to solve the _Sum Over Threshold_ problem.
 * Demonstrates a race condition when run with multiple threads, yielding non-deterministic answers.
 * Also introduces the use of OpenMP to trivially parallelise a for loop.
 */
template < typename T >
    T sum_over_threshold( T const threshold )
    {
        auto const n = sizeof( input_data ) / sizeof ( input_data[ 0 ] );
        auto sum = 0lu;

        // We parallelise the loop with OpenMP because of its simplicity: it allows us to
        // focus on concepts for _exposing parallelism_, rather than on details of managing
        // thread pools. Alternatives include Intel Thread Building Blocks (TBB), pthreads
        // and the newer parallel features built directly into C++11 and later.
        // Threading is enabled with this single pragma line, assuming that the program is
        // linked with the `-fopenmp` flag and the `<omp.h>` header file is included.
        //
        // `#pragma omp parallel` denotes a section of code that should be run in parallel
        // `#pragma omp for` denotes that a for loop should be split among all threads
        // `#pragma omp parallel for` is shorthand for combining both.
        // If the number of threads is _t_, this splits the range [0,...,n-1] into _t_
        // equal-sized partitions and assigns one to each thread.
        // This "workload schedule" is done "statically" at compile-time.
        // One can also specify `num_threads( x )` to explicitly determine the number of
        // threads that should cooperate on this loop as _x_; by default, it is the
        // value of `omp_get_max_threads()`, which can be set with `omp_set_num_threads()`.
        #pragma omp parallel for schedule( static )
        for( auto i = 0lu; i < n; ++i )
        {
            // Race condition!!
            // Here multiple threads will try to update `sum` at the same time, and
            // the exact order of reading the previous value, performing the addition,
            // and writing the new value can (incorrectly) interleave between threads.
            // This produces non-deterministic, incorrect results.
            // A major part of _exposing parallelism_ is designing around contention like this.
            // The high-cost, default approach to solving this is to *lock* `sum`, e.g., with
            // a _mutex_. We want to avoid locks in this course, as they _can_ destroy throughput.
            sum += branchless_sum< T >{ threshold }( input_data[ i ] );
        }

        return sum; // Non-deterministic!
    }
} // namespace race_condition



namespace manual_parallel_reduction {

/**
 * Introduces the concept of a _parallel reduction_ to avoid a race condition
 * with minimal overhead and no locking.
 */
template < typename T >
    T sum_over_threshold( T const threshold )
    {
        auto const n = sizeof( input_data ) / sizeof ( input_data[ 0 ] );

        // A **parallel reduction** spawns multiple copies of a shared, mutable variable
        // so that each thread has its own local copy. When all threads are finished,
        // the local copies are aggregated together. This introduces the small overhead
        // of maintaining more state and of the processing cost to combine the local results;
        // however, it completely avoids locking or synchronisation (until the end of the loop).
        //
        // We enable this here by creating a vector of sum variables, one for each thread.
        // Each thread then computes the local sum of all values it reads in its own local thread.
        // Because summation is an associate and commutative operation, we can simply sum
        // the local sums to get the global answer.
        auto const num_threads = omp_get_max_threads();
        std::vector< uint64_t > sum( num_threads, 0lu );

        #pragma omp parallel for
        for( auto i = 0lu; i < n; ++i )
        {
            // `omp_get_thread_num()` gets the id of the current thread (which are numbered
            // sequentially from zero). We can use the thread id to index into the `sum` array
            // to obtain a unique (local) counter for this thread.
            auto const t = omp_get_thread_num();
            sum[ t ] += branchless_sum< T >{ threshold }( input_data[ i ] );
        }

        // "Reduce" the local results by summing them together
        return std::accumulate( std::cbegin( sum ), std::cend( sum ), 0lu );
    }
} // namespace manual_parallel_reduction



namespace auto_parallel_reduction {

/**
 * Parallel reductions are so common that they are built directly into OpenMP.
 * This final version is equivalent to that in the manual::parallel::reduction::
 * namespace, but uses OpenMP to perform the reduction rather than writing it
 * explicitly.
 */
template < typename T >
    T sum_over_threshold( T const threshold )
    {
        auto const n = sizeof( input_data ) / sizeof ( input_data[ 0 ] );
        auto sum = 0lu; // Single global shared mutable variable again

        // We provide the `reduction( +: sum )` clause in the pragma
        // This creates a parallel reduction (as we did above), using
        // addition/summation as the operation (both locally *and* globally)
        // and using `sum` as the variable into which to record the answer
        #pragma omp parallel for reduction( +:sum )
        for( auto i = 0lu; i < n; ++i )
        {
            // No race condition, because OpenMP will localise this per thread
            sum += branchless_sum< T >{ threshold }( input_data[ i ] );
        }

        return sum; // simpler; aggregation already done by OpenMP
    }
} // namespace auto_parallel_reduction


int main( int argc, char ** argv )
{
    auto sum = 0lu;
    auto const benchmark_trials = 20000u;
    auto const threshold = 750llu;

    if( argc >= 2 )
    {
        // omp_set_num_threads() sets the global number of threads used by OpenMP
        // If this is not set, then it defaults to the number of cores on the machine
        // Here, we take the value from the first command line argument (`argv[ 1 ]`),
        // after converting it from ascii to int (the `atoi()` function).
        omp_set_num_threads( atoi( argv[ 1 ] ) );
    }

    auto const start_time = std::chrono::system_clock::now();

    for( auto i = 0u; i < benchmark_trials; ++i )
    {
        // Toggle example by changing namespace
        sum += race_condition::sum_over_threshold( threshold );
        // sum += manual_parallel_reduction::sum_over_threshold( threshold );
        // sum += auto_parallel_reduction::sum_over_threshold( threshold );
    }

    auto const end_time = std::chrono::system_clock::now();
    auto const elapsed_time = std::chrono::duration_cast< std::chrono::microseconds >( end_time - start_time );

    std::cout << "sum = " << ( sum / static_cast< float >( benchmark_trials ) ) << std::endl;
    std::cout << "average time per run: "
              << elapsed_time.count() / static_cast< float >( benchmark_trials )
              << " us" << std::endl;

    return 0;
}
