# Introduction to Multi-core (via OpenMP) and Parallel Reductions

## Overview

### Multi-core Processing
Multi-core data processing refers to the use of multiple processing cores to cooperatively solve a data processing task. The unique aspect of multi-core architectures relative to (shared-nothing) distributed systems is that the cores share memory and, for some, even last-level cache. This offers a distinctive advantage that we can exploit, because it cheapens the cost of communication and reduces rendundant storage. However, it is also difficult to design for, and many multi-core algorithms simply adopt a shared-nothing approach, squandering that potential.

The main challenge in a shared-memory system is that concurrent threads may try to access the same resources, leading to contention. This can be, for example, contention for LL cache space or for who gets to write first to a particular address. Most often, this is handled with mutual exclusion (mutex) locks, where a thread can only access a resource if no other threads have acquired locks on it. Of course, this _serialises_ code, as threads must _wait_ for each other. In general, we want to _design_ our multi-core applications so that threads wait as little as possible for each other. This course will focus on lock-free techniques.

### Threading Libraries
There are several libraries available for threading that each offer different advantages:
  
  * pthreads (the standard UNIX threading model, which is manipulated with c-like functions)
  * OpenMP (a set of directives that instruct the compiler how to parallelise the code)
  * Intel Thread Building Blocks (especially compatible with the Intel VTune software)
  * C++17 (a recently introduced set of parallel extensions to the C++ standard template library)

We will focus on OpenMP because it is *very* simple to get started with. This lesson, for example, only requires 2-3 lines of code, one header file `#include`, and one extra compiler/linker flag. We want to focus on concepts of (exposing) parallelism, not threading syntax.


## Problem Definition

In this task, we tackle the _Sum Over Threshold_ problem already introduced in the _Branch Prediction_ lesson. Specifically,
we are given a vector of numbers, `v = < v_i >` and a threshold `t`, and we want to calculate the sum over all elements in `v` that exceed the threshold:

```
SOT( v, t ) = ∑_{v_i ∈ v, v_i ≥ t}
```

However, this time we want to apply multi-core parallelism to accelerate our branch-free solution.

## Code Instructions

There are two files in this lesson:
  
  * `openmp.cpp`: the implementation of a multi-core _parallel reduction_ for the _Sum Over Threshold_ problem
  * `unsorted-data.hpp`: a static array of numbers, generated uniformly at random

This lesson introduces OpenMP. To compile the code, you will need to link against the OpenMP library using the `-fopenmp` flag.

> g++ -Wall -O3 -std=c++17 -mavx -march=native -fopenmp openmp.cpp -o omp

If running _Clang_ instead of _gcc_, you will also need to specify an include path. See the Clang/LLVM blog for details: http://blog.llvm.org/2015/05/openmp-support_22.html.


## Design Sequence

We add command line input handling in this lesson so that we can change the number of threads each time we run the application. The usage is:

> ./omp [num_threads]

The last command is optional. If excluded, the default number of threads is chosen (equal to the number of logical cores on the machine.)


### Naive Parallelisation

Our first live-coded solution is given in namespace `naive::`. We simply add a `#pragma omp parallel for` pragma to our _hot_ for loop. This is a compile-time work-sharing directive that splits the loop into equal, contiguous parts for each thread; e.g., if there are two threads, then the first does iterations [0,n/2) and the second thread concurrently does iterations [n/2,n).

We first benchmark the implementation on a single thread, using:

> ./omp 1

This produces output similar to:

```
sum = 6.54981e+08
average time per run: 1046.65 us
```

We can then evaluate the speed-up we get from adding an additional core:

> ./omp 2

This produces output similar to:

```
sum = 3.27386e+08
average time per run: 693.407 us
```

There are two key observations:

 1. The run-time improved by a factor of 1046.65/693.407 = 1.51×. It's not 2.0×, but it's also not bad for having only added two lines of code!
 2. The result is different from the single-threaded run...

In fact, if we run it again, we observe output similar to:

```
sum = 3.27453e+08
average time per run: 687.313 us
```

Now we note that the results aren't even the same under the same configuration; they are non-deterministic! This is because we have introduced a classic multi-threading error: a race condition. This occurs when multiple threads try to access the same value concurrently and at least one of them is trying to overwrite it with a new value. The precise result depends on the (random) order in which the threads execute their instructions.


### Parallel Reduction

We solve the race condition in namespace `manual_reduction::`. This is a common template for parallel programming that circumvents race conditions without locking. The main idea is that each thread computes a local answer over all the values that it has seen and only when every thread has finished we combine their results. For any operation that is associative (e.g., sum, count, max, and min, but not average), a parallel reduction can apply.

We hand-code the parallel reduction by creating a vector of temporary solution values. Each thread uses only the solution value at the index corresponding to its thread id. After the loop ends (where there is an implicit synchronisation barrier), we return the sum of the array elements.

After recompiling in the new namespace, we can again assess single-threaded performance:

> ./omp 1

We observe results similar to:

```
sum = 6.54981e+08
average time per run: 1027.9 us
```

This is equivalent to the previous implementation, so we have not introduced much overhead with the parallel reduction. On two core, we observe results similar to:

```
sum = 6.54981e+08
average time per run: 695.822 us
```

Now the result is correct, and we still obtain a ≈1.5× speed-up.


### OpenMP Parallel Reduction

The parallel reduction is such a common pattern, that it is implemented as part of the OpenMP standard. We achieve the same results as above in namespace `auto_reduction::`, but by instead using the `reduction ( +:sum )` clause. However, now we know what this clause is doing and we can apply the pattern to more complicated examples that are not supported by OpenMP or other threading libraries, e.g., non-standard functions on non-primitive types.

## Final Notes

 * You may wonder why we only achieved about a ≈1.5× speed-up? One explanation could be a phenomenon called "false sharing": even though the threads write to separate variables, they do not write to separate cache lines!
 * The _parallel reduction_ pattern more generally refers to taking a large array of input and repeatedly "reducing" it _t_-fold at each iteration (where _t_ is the number of threads). This solves the problem in `log_{t}(n)` steps. However, in this lesson, we collapse all but the last iteration into one, as they are done locally to a single thread and the final aggregation is too cheap to parallelise.


## References

[1] OpenMP Architecture Review Board (2019) "OpenMP 5.0 API Syntax Reference Guide." https://www.openmp.org/wp-content/uploads/OpenMPRef-5.0-0519-web.pdf Accessed 8-Mar-2020.
