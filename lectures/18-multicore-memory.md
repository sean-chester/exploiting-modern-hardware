# Memory Model for (Intel) Multicore Machines

## Objective

_Discuss the multi-socket NUMA memory model and its implications for achieving parallel scalability_


## Intro/Layout

 * Question: who has heard of "NUMA"? : non-uniform memory access
   + Contrast concept to the RAM computational model
   + Actually, studying cache locality has already provided a notion of "NUMA", but generally refers to something more specific


## L3 Cache Slices

 * Complete picture of cache and memory (using reputable sources)
   + Figure 2-3 [1], p.48: cache structure for multicore
     - observe *shared* l3
     - observe differences between Skylake (examples in class) and Broadwell (experiments server)
       * L2 cache size
       * L3 smaller but non-inclusive in Skylake
     - Note `size * N`!!
       * Each core appends another 1.375MB of L3 cache
       * Figure 2.8 [1], p.60: ring interconnect between l3 slices (one per core)
         + latency over ring is negligible compared to total l3 latency


## NUMA/QPI Link/Multisocket

  * how to add more than one "ring-worth" of processors? E.g., what does a 32-core machine look like?
    + Figure 1 [2], p.3: simplified memory layout for multi-socket
      - Note IMC connects to memory, separate memory on each quad-core ring
      - Can access memory on other ring, but have to go through its IMC and the QPI link first
        * Adds bandwidth to other core's memory controller
    + Figure 2-9 [1], p.65: more detailed, 3 x quad-core
  * Question: what does this mean to your program?
    + Figure 4,5 [2], p.5: thread affinity (keeping work local to a core)
      - Contrast vs shared-nothing, depends on workload 


## Cache Coherency

  * Question: what about *consistency* in the thread-local L2 caches and the L3 cache slices?
    + If two cores read/write data at same address, they each have a local copy in their L2 cache
      - In presence of writes, these can become out-of-sync, compromise unified memory model
      - Cache coherency = guaranteeing sequential equivalency + consistency across cores
        * On Intel, use directory in memory: each time cache line is modified, have to write to directory
          + Even just reads must check directory for writes in cache line is co-held
          + High latency to read to memory
          + Alternative = snooping: each core confirms with other core's cache
  * Best way to avoid this?
    + Try to only write to thread-local data; make shared data const/immutable
    + If writing to shared data, try to cluster writes
    + Try not to read random data from other cores
    + I.e., Design for locality, use const liberally


## References

[1] Intel (2019). _IntelÂ® 64 and IA-32 Architectures Optimization Reference Manual_. https://software.intel.com/en-us/download/intel-64-and-ia-32-architectures-optimization-reference-manual

[2] Porobic et al. (2012) "OLTP on Hardware Islands." _PVLDB_ 5(11). http://vldb.org/pvldb/vol5/p1447_danicaporobic_vldb2012.pdf
