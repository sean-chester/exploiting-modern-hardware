# Branch Prediction and Branch-free Code

## Overview

On a modern, super-scalar processor with out-of-order execution, performance depends on the processor being able to look several instructions ahead to populate a long pipeline of in-progress instructions. This is necessary to hide latencies.

However, what can the processor do if one of the upcoming instructions is a branch? For example, consider this code:

```
int x << cin;
if( x > 10 )
{
	// do something
}
```

There is a _branch_, i.e., what code should be run depends on a condition. This is a particularly "nasty" condition, too, because it depends on user input; i.e., it is very difficult even for the programmer to predict whether the code inside the if statement should be executed.

In terms of the pipeline, should the processor wait for the user input, begin executing the code inside the if statement (and hope the user input is less than 10), or begin executing the code after the if statement under the assumption that the if-body will be skipped? All of these alternatives involve some degree of speculation. "Bad" (i.e., incorrect, irrespective of whether a correct guess was possible) speculation requires undoing all the work that was speculatively executed based on the prediction; i.e., a "machine clear."

This can be tremendously expensive, and that's why "Bad Speculation" is one of the top-level concerns in the Intel top-down methodology. For example, the result of bad speculation could be a loss of spatial and/or temporal locality, which in turn creates cache misses. If we look at cache misses without considering speculation (i.e., deviate from the top-down methodology) we could spend a lot of time optimising data structures when the problem is actually control flow.

This is described excellently in a very highly upvoted [StackOverflow response](https://stackoverflow.com/a/11227902/2769271) [2]. This lesson analyses the claims in that response (and the question it responds to) to determine if branch prediction is still worth considering nine years later.


## Code Instructions

There are two programs in this lesson, a data generator and the main example:
  
  * `data-generator.cpp` (run this first)
  * `branch-pred.cpp` (change this code directly and recompile for alternative experimental configurations)

### Data Generation

The data generation is done in a separate application in order to profile the experiments as a stand-alone application, without measuring the overhead of the experiment set up. To generate the data, first build the data generator:

> g++ -Wall -O3 -mavx -march=native -O3 data-generator.cpp -o data-gen

And then run it with an input size, to indicate how large should be the arrays in your experiments. In class, we used 10^6, e.g.:

> ./data-gen 1000000

This will create two header files, `sorted-data.hpp` and `unsorted-data.hpp` which declare static test data for the experiments.


### Branch Prediction Experiments

The main code demonstration today is in `branch-pred.cpp`. To compile with optimisation, try:

> g++ -Wall -O3 -mavx -march=native branch-pred.cpp -o [your-binary-name]

To compile without optimisation, you can change `-O3` to `-O2`, `-O1`, or `-Og`. (`-O0` turns off all the debug symbols as well; so, it can be harder to profile your code at that optimisation level.) Then run as:

> ./[your-binary-name]

To change experiment configurations, edit the source file, `branch-pred.cpp`, recompile, and rerun.


## Profiling Instructions

This lesson is set up to interact very easily with the Linux `perf` tool.
The default behaviour of `perf stat` is to print out (among other things) branch prediction statistics.
Depending on the configuration of your machine, you may need root access to read counters.
Assuming you compiled the code into a binary called `branch-pred`, you can profile the code as:

> sudo perf stat ./branch-pred

With unsorted, branching STL code, (and depending on your architecture and compiler version) you should see results similar to:

```
sum = 749724114000
average time per run: 4653.37 us

 Performance counter stats for './bp':

          9,307.53 msec task-clock                #    1.000 CPUs utilized          
                25      context-switches          #    0.003 K/sec                  
                 1      cpu-migrations            #    0.000 K/sec                  
               192      page-faults               #    0.021 K/sec                  
    27,518,315,457      cycles                    #    2.957 GHz                    
    17,017,079,546      instructions              #    0.62  insn per cycle         
     4,002,594,656      branches                  #  430.038 M/sec                  
     1,004,383,548      branch-misses             #   25.09% of all branches        

       9.308431884 seconds time elapsed

       9.307872000 seconds user
       0.000000000 seconds sys
```

Observe in particular that there were roughly 4 billion branches of which roughly 1 billion (25 %) were mispredicted.
You may also note that there were only 17 billion instructions in total; so, nearly 25 % of this program is spent on branches.
In general, a branch misprediction ratio over a few percent is quite bad and indicates a probable bottleneck to optimise,
at least if branches and/or branch-dependent code make up a large percentage of what has been profiled.

25 % = (0.5 * 0 %) + (0.5 * 50 %) should match what you expect. Every iteration through the loop has two branches:

 * should the loop continue for another iteration (only mispredicted on the very last iteration)
 * should the next value be added to the running sum (predicted with about 50 % accuracy)

## What to Try

In class, we tried several combinations in order to better understand the effect on branch prediction and the implications that had for running time. Below are 2^4 possible scenarios to explore (and perhaps you can think of more!).

 * Compile at full optimisation (-O3) versus (-Og)
   + In class we saw that -O3 eliminated the branch. As per the StackOverflow response and the Intel Optimisation Guide, newer machines have a CMOV ("conditional move") instruction (with 2-cycle latency) that can eliminate branches. This is likely what we observed in class, and could be confirmed using, e.g., https://godbolt.org/ or gcc disassembly.

 * Alternate between STL (`std::accumulate()`) and raw, c-style for loops
   + In class we used for loops, as in the StackOverflow post. This code instead uses modern style (STL and functional programming). In post-class tests, I observe the STL implementation to run faster but to not optimise away the branch, even with -O3.

 * Alternate between sorted and unsorted data
   + This directly tests (the currency of) the observation in the StackOverflow question, that presorted order will run faster
   + This fits perfectly with our expectations of branch prediction (a predictable pattern at branches, driven by the data), but could be irrelevant if techniques have been applied (by the programmer and/or the compiler) to eliminate the branch

 * Alternate between a programmatic branch-free and branching implementation (i.e., alternate which functor is used)
   + In class we also tried this, and observed as good performance from the branch-free code as from pre-sorting the data, but only when running on a lower level of optimisation. You may observe slightly different results with the STL implementation.

A systematic approach to optimisation would test out all sixteen combinations. (It only takes 10 seconds to run the slowest configuration.)

You could also try augmenting the successful combinations with techniques from previous lectures, e.g., loop unrolling to improve instruction-level parallelism, to see if the bottleneck has shifted away from branch prediction and enabled other optimisations again.


## Additional Tips

There are several ways as a programmer that you can aid the branch predictor. A few common tips include:

  * Other logical manipulations, such as in [2]
    + However, be careful with these, as they may be slower than CMOV or other compiler-level optimisations

  * Put your most likely case in the `if` rather than the `else`
    + As per [1], the compiler will guess not to jump at a branch when it has no idea. Compilers are likely to jump to your else rather than your if.

  * Consider doing "extra" work
    + Sometimes we use branches to skip edge cases in an effort to do less computation; however, this introduces a lot of work in terms of branches and branch prediction. It is worth profiling how often you really skip that work and considering just doing it anyway in order to have branch-free code

Note that the GPU computational model is inherently branch-free. So, branch-free code is also much more likely to port to a GPU.


## References

[1] Intel. (2019) "Section 3.4.1: Branch Prediction Optimization." In _IntelÂ® 64 and IA-32 Architectures Optimization Reference Manual_. https://software.intel.com/en-us/download/intel-64-and-ia-32-architectures-optimization-reference-manual

[2] "Mysticial" (2012). "You are a victim of branch prediction fail." _StackOverflow_. https://stackoverflow.com/a/11227902/2769271