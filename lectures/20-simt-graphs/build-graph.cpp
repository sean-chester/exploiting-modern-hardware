/**
 * Alternative methods to transform a graph from an unsorted list of edges
 * into a flattened adjacency list format in parallel. Illustrates SIMT-friendly
 * algorithm re-design.
 */

#include <chrono>             // timing library
#include <algorithm>          // std::sort(), std::lower_bound()
#include <parallel/algorithm> // __gnu_parallel::sort()
#include <iostream>

#include <vector>

#include "graph.hpp" // data structures and types for this lesson

namespace { // anonymous

#include "edges.hpp" // statically generated vector of edge structs
auto const m = edges.size();

} // namespace anonymous


namespace sequential {

/**
 * Scans the input list of edges to determine the largest vertex id.
 */
vertex get_max_vertex()
{
    vertex max_vertex = 0;
    for( auto const e : edges )
    {
        max_vertex = std::max( max_vertex, e.u );
        max_vertex = std::max( max_vertex, e.v );
    }
    return max_vertex;
}

/**
 * Returns a degree sequence of the input edge list; i.e., a vector
 * in which every index i gives the out-degree of vertex i.
 */
degree_sequence get_degree_sequence( vertex const n )
{
    degree_sequence deg_seq( n, 0u );
    for( auto const e : edges )
    {
        ++deg_seq[ e.u ];
    }
    return deg_seq;
}

/**
 * Calculates the prefix sum of a vector of integers; i.e.,
 * every index i stores the sum of all values that precede it.
 * Does an in-place transformation and returns the transformed
 * sequence by value. Should be used in contexts with guaranteed RVO.
 *
 * For example, the input vector <3,1,4,0,2>
 * would be transformed into vector <0,3,4,8,8>
 */
degree_sequence prefix_sum( degree_sequence deg_seq )
{
    vertex sum = 0;
    for( auto i = 0lu, n = deg_seq.size(); i < n; ++i )
    {
        std::swap( sum, deg_seq[ i ] );
        sum += deg_seq[ i ]; // adds previous running sum
    }

    return deg_seq;
}


/**
 * Construct a standard adjacency list (vector of vectors) in a
 * straight-forward fashion. The obvious sequential baseline.
 */
adj_list to_adjacency_list()
{
    // Determine the number of vertices, n, and then allocate
    // n neighbour lists.
    auto const n = 1u + get_max_vertex();
    adj_list neighbours( n );

    // For each edge, push its destination onto the relevant
    // source vertex's neighbour list.
    for( auto const e : edges )
    {
        neighbours[ e.u ].push_back( e.v );
    }

    return neighbours;
}

/**
 * Transforms unsorted input edge list into a parallel-friendly *flattened* adjacency
 * list data structure.
 *
 * Overall approach is to first determine the "shape" of the data structure; i.e.,
 * how much of the neighbour list should be reserved for each vertex. This is done
 * with a first pass over the edge list to determine the degree of every vertex.
 * Then, the degree sequence is converted to its prefix sum, which gives an offset
 * for every node into the edge list array. Finally, the edge destinations can be
 * directly copied into the correct position of the neighbours vector, because the
 * offsets for each source are known.
 * @see graph.hpp for details on the data structure.
 */
flat_adj_list to_flat_adjacency_list()
{
    auto const n = 1u + get_max_vertex();

    auto const node_offsets = prefix_sum( get_degree_sequence( n ) );
    std::vector< vertex > neighbours( edges.size() );

    // Indicates how many neighbours have been added for each vertex.
    // node_offsets only indicates where the list of neighbours for each
    // vertex *begins*; curr_offsets indicates how far into that list we
    // are for each vertex
    degree_sequence curr_offsets( n, 0u );

    for( auto const e : edges )
    {
        // Observe: node_offsets[ e.u ] + curr_offsets[ e.u ] gives the index of the
        // next as-yet-unpopulated index for neighbours of vertex e.u
        neighbours[ node_offsets[ e.u ] + curr_offsets[ e.u ]++ ] = e.v;
    }

    return { node_offsets, neighbours };
}


} // namespace sequential
namespace parallel {

/**
 * Parallel method to set `max_vertex` to the largest vertex id
 * seen in the input edge list.
 * @pre max_vertex should be initialised to 0 and should be globally shared
 */
void set_max_vertex( vertex & max_vertex )
{
    // Use standard parallel reduction, with aggregation function `max`.
    #pragma omp for reduction( max:max_vertex )
    for( auto i = 0lu; i < m; ++i )
    {
        max_vertex = std::max( max_vertex, edges[ i ].u );
        max_vertex = std::max( max_vertex, edges[ i ].v );
    }
}

/**
 * Parallel method to populate deg_seq with the degree sequence of the
 * input edge list. Straight-forward adaptation of `sequential::get_degree_sequence()`
 * using atomic increments. Allocates space for deg_seq.
 */
void populate_degree_sequence( degree_sequence & deg_seq, vertex const n )
{
    #pragma omp single
    deg_seq = degree_sequence( n, 0 );

    #pragma omp for
    for ( auto i = 0lu; i < m; ++i)
    {
        __sync_fetch_and_add( &( deg_seq[ edges[ i ].u ] ), 1 );

    } // implicit barrier
}

/**
 * Transforms offsets from a degree sequence to a prefix sum by simply
 * invoking the non-parallel `sequential::prefix_sum()` method. Note here
 * the implications in terms of Amdahl's Law.
 */
void calculate_offsets( degree_sequence & offsets, vertex const n )
{
    #pragma omp single
    offsets = sequential::prefix_sum( degree_sequence( offsets ) );
}

/**
 * Populates the pre-allocated neighbours list, based on the shape determined in offsets.
 * A straight-forward parallelisation of the code in `sequential::to_flat_adjacency_list()`
 * using atomic increments.
 * @note curr_offsets is a globally-shared length-n vector, set to all zeroes. It indicates
 * how many neighbours for each vertex have already been populated into neighbours.
 */
void populate_neighbour_list( auto & neighbours, auto const& offsets, auto & curr_offsets, vertex const n )
{
    #pragma omp single
    curr_offsets = degree_sequence( n, 0 );

    #pragma omp for
    for( auto i = 0lu; i < m; ++i )
    {
        auto const u = edges[ i ].u;
        auto const v = edges[ i ].v;

        // offsets[ u ] + u_idx gives the first index of neighbours of u that has not already
        // been populated; i.e., we are emulating `std::vector<>::push_back().` 
        auto const u_idx = __sync_fetch_and_add( &( curr_offsets[ u ] ), 1 );
        neighbours[ offsets[ u ] + u_idx ] = v;

    } // implicit barrier
}

/**
 * Transforms an unsorted edge list into a flattened adjacency list data structure in parallel.
 * Follows the same general approach as `sequential::to_flat_adjacency_list()`, but uses atomics
 * to synchronise threads.
 */
flat_adj_list to_flat_adjacency_list()
{
    vertex max_v = 0;
    degree_sequence offsets, curr_offsets;
    std::vector< vertex > neighbours( m );

    #pragma omp parallel
    {
        set_max_vertex( max_v );
        populate_degree_sequence( offsets, max_v + 1 );
        calculate_offsets( offsets, max_v + 1 );
        populate_neighbour_list( neighbours, offsets, curr_offsets, max_v + 1 );

    } // parallel section

    return { offsets, neighbours };
}

/**
 * Calculates a flattened adjacency list in a SIMT-friendly manner, but
 * with O(m + n lg n) serial complexity.
 *
 * Reorganises computation so that threads obtain ownership of edges and
 * vertices without synchronisation by first sorting the input by source
 * vertex. Thereafter, the edges are in the same order as the final neighbour
 * list and can be straight-forwardly copied/transformed. The node offsets
 * can be calculated by launching a binary search for every vertex to determine
 * where its neighbours begin. Both of these are independent (i.e., we can
 * use the `nowait` OpenMP clause) and expose fully independent tasks/ownership.
 */
flat_adj_list sort_first()
{
    vertex n = 0u;
    degree_sequence offsets;
    std::vector< vertex > neighbours( edges.size() );

    // The key is incurring this cost up front, which permits the algorithmic
    // changes below.
    __gnu_parallel::sort( std::begin( edges )
                        , std::end  ( edges )
                        , []( auto l, auto r ){ return l.u < r.u; } );

    #pragma omp parallel
    {
        set_max_vertex( n );

        #pragma omp single
        offsets = degree_sequence( n + 1 );

        // transform the sorted edge list into our neighbour list.
        // observe this is now a straight-forward copy, because the sort orders match
        // The `nowait` clause indicates that this is a *non-blocking* work-sharing
        // construct; i.e., the implicit synchronisation barrier at the end of the loop
        // is removed and threads can move onto the next loop as soon as they are ready;
        // the second loop does not depend on the results of this loop.
        #pragma omp for nowait
        for( auto i = 0u; i < m; ++i )
        {
            neighbours[ i ] = edges[ i ].v;
        }

        // For each vertex in parallel, determine where it's neighbours start with
        // a simple binary search over the list of edges. Like above, this is possible
        // because the sort order for the edges now matches the sort order for our final
        // flattened adjacency list data structure.
        #pragma omp for
        for( auto i = 0u; i <= n; ++i )
        {
            offsets[ i ] = std::distance( std::cbegin( edges )
                                        , std::lower_bound( std::cbegin( edges )
                                                          , std::cend  ( edges )
                                                          , i
                                                          , []( auto edge, auto u ){ return edge.u < u; } ) );
        }
    } // end parallel section

    return { offsets, neighbours };
}

} // namespace parallel



int main()
{
    auto sum = 0.0;
    auto const num_trials = 2000u;

    auto const start_time = std::chrono::system_clock::now();

    for( auto i = 0u; i < num_trials; ++i )
    {
        sum += sequential::to_adjacency_list().back().size(); // different structre = different result
        // sum += sequential::to_flat_adjacency_list().node_offsets.back();
        // sum += parallel::to_flat_adjacency_list().node_offsets.back();
        // sum += parallel::sort_first().node_offsets.back();
    }

    auto const end_time = std::chrono::system_clock::now();
    auto const elapsed_time = std::chrono::duration_cast<std::chrono::microseconds>( end_time - start_time );

    std::cout << "answer: " << ( sum  / static_cast< float >( num_trials ) ) << std::endl;
    std::cout << "time: " << ( elapsed_time.count() / static_cast< float >( num_trials ) ) << " us" << std::endl;
    return 0;
}
