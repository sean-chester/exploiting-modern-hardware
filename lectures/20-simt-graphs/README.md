# SIMT-Friendly Graph Construction

## Overview


## Problem Definition

Our task for this lesson is to **transform an edge list into an adjacency list**. Many graph algorithms involve repeatedly traversing all the neighbours of a given vertex, and an adjacency list layout is optimised for this traversal pattern. 

Let `E = {(u_i,v_i)}` be a set of edges, i.e., ordered pairs of vertices from a vertex set `V`. We wish to transform it into a data structure that maps each source vertex `u_i` to a contiguous list of all its incident vertices `v_i`, i.e., an _adjacency list_.

Specifically, we would like to conduct this transformation in a highly parallel, SIMT-friendly way.


## Code Instructions

There are three files in this lesson:
  
  * [graph.hpp](graph.hpp): a header file that defines all the graph data structures and types used in this lesson
  * [build-graph.cpp](build-graph.cpp): an implementation of several algorithms, sequential and parallel, to transform an edge list into an adjacency list
  * [edges.hpp](edges.hpp): a static vector of edge structs, drawn uniformly at random from the space of possible ordered pairs of vertices

To compile the code, you will need the `-fopenmp` linker flag to enable parallelisation with OpenMP:

> g++ -Wall -O3 -std=c++17 -fconcepts -fopenmp -mavx -march=native build-graph.cpp -o bg


## Design Sequence

We have gone through the following steps to solve this problem:

### (Naive) Single-threaded solution

As usual, we begin by establishing a baseline solution so that we can measure the overhead and work-efficiency of our parallel solutions. This is given in `sequential::to_adjacency_list()`, which is a standard (sequential) approach. We simply scan the dataset to determine the number of vertices, _n_, and initialise an adjacency list as _n_ empty vectors. We then scan the dataset a second time and push all the neighbours onto the back of the appropriate vectors.

#### Profiling/Analysis

If we run the code (pinned to a specific core) on a 6-core Broadwell Azure instance:

> numactl --physcpubind=0 ./bg

we observe results similar to:

```
answer: 477
time: 5074.29 us
```

Note that the "answer" is just the degree of the last vertex, an arbitrarily chosen, lightweight indicator for correctness. This establishes a baseline performance of about 5 milliseconds for transforming this edge list into an adjacency list. However, the adjacency list is in a vector-of-vectors format, which is not ideal for the GPU. Instead, we would like to create a "flattened adjacency list."


### Flattened Output Structure

Building a _flattened adjacency list_ (described in [graph.hpp](graph.hpp)), without first building a vector-of-vectors-style adjacency list requires some ingenuity, because we have to pre-allocate the space for each vertex's list of neighbours. That is to say, in the flat, long list of neighbours, we need to know where the neighbours of each vertex begin and end. That is to say again, we need to know the "shape" of the data structure before we can start populating it.

Thus, we end up with a three-pass algorithm now. The first pass determines the number of vertices as before. But now the second pass has to determine the range where each vertex's neighbours will be inserted: for vertex _i_, the width of the range is exactly the degree of `v_i`. Where the range begins is the prefix sum of degrees, i.e.:

`âˆ‘_{j=0,...,i-1} deg(v_j)`

The second pass therefore calculates all the degrees of all the vertices and thereafter the prefix sum of that degree sequence. Finally, the third pass can place the neighbours into the correct spots. This is described in `sequential::to_flat_adjacency_list()`.

#### Profiling/Analysis

Running the code, we observe performance similar to:

```
answer: 499523
time: 5312.7 us
```

Amazingly, despite the extra pass over the dataset, this algorithm runs equally fast. Probably, this is because of the lack of re-allocations incurred by `std::vector<>::push_back()`, but it could also result from slightly less pointer-chasing in the flattened data structure. Generally, it would be worth answering this question directly, but we are not stopping at this point, anyway. We need to parallelise the algorithm, still!

Note that the "answer" changed because we have a different data structure now. This time we are reporting the index in the `neighbours` array where the last vertex's neighbours begin. Indeed, this matches the previous result, as the sum of these numbers should be _m = 500000_.


### Parallelisation of Method 2

Each of the steps in the algorithm to produce a flattened adjacency list can be individually parallelised if we manage all contention with [atomics](../16-atomics/). Each pass over the dataset was a for loop to which we can apply the OpenMP `parallel for` work-sharing construct. Data races should be relatively infrequent, because each subsequent edge will be appended to a randomly chosen vertex's neighbour list. This approach is given in the `parallel::` namespace.

#### Profiling/Analysis

We run the code pinned to a single core to measure the overhead of the synchronisation and see results similar to:

```
answer: 499523
time: 10985.5 us
```

The answer is correct, but the overhead was nearly double. Worse yet, if we run it in parallel on all six cores, we observe:

```
answer: 499523
time: 13054.4 us
```

It's a negative speed-up and we're nearly thrice slower than the sequential baseline! (And this is where we left off the live-coding in class.)

Returning to the single core run, we can start to understand the problem better with perf and the Intel topdown methodology. (_Note that "perf" cannot be run on the Azure instance, so this is taken from my Skylake laptop_.)

> sudo perf stat -a --topdown numactl --physcpubind=0,1 ./bg

We observe something similar to:

```
                              retiring      bad speculation       frontend bound        backend bound 
S0-C0           2                 6.1%                 0.2%                 1.2%                92.5% 
S0-C1           2                 8.2%                 0.8%                 3.3%                87.7% 
```

The algorithm appears to be heavily memory-bound, as we can confirm with:

> sudo perf stat -d numactl --physcpubind=0 ./bg

```
answer: 499523
time: 11718.1 us

 Performance counter stats for 'numactl --physcpubind=0 ./bg':

         23,452.59 msec task-clock                #    1.000 CPUs utilized          
                73      context-switches          #    0.003 K/sec                  
                 1      cpu-migrations            #    0.000 K/sec                  
         1,897,318      page-faults               #    0.081 M/sec                  
    69,760,027,724      cycles                    #    2.975 GHz                      (49.98%)
    28,900,141,401      instructions              #    0.41  insn per cycle           (62.48%)
     3,246,991,757      branches                  #  138.449 M/sec                    (62.50%)
         3,472,603      branch-misses             #    0.11% of all branches          (62.52%)
    13,861,788,114      L1-dcache-loads           #  591.056 M/sec                    (62.53%)
     1,345,507,526      L1-dcache-load-misses     #    9.71% of all L1-dcache hits    (62.51%)
       175,375,148      LLC-loads                 #    7.478 M/sec                    (49.99%)
        74,432,381      LLC-load-misses           #   42.44% of all LL-cache hits     (49.98%)

      23.441472763 seconds time elapsed

      20.967664000 seconds user
       2.485382000 seconds sys
```

About 10% of level-1 data cache accesses and 30% of last-level L3 cache (LLC) accesses are misses and thus the IPC is less than 1. As we've learned, trying to parallelise a memory-bound application is only going to stress the memory sub-system. For example, running 2 cores instead of 1 means that there is only half as much LLC available per thread. Here are the perf stats running on both cores:

```
answer: 499523
time: 17879.1 us

 Performance counter stats for 'numactl --physcpubind=0,1 ./bg':

         71,250.57 msec task-clock                #    1.992 CPUs utilized          
               213      context-switches          #    0.003 K/sec                  
                 2      cpu-migrations            #    0.000 K/sec                  
         1,895,325      page-faults               #    0.027 M/sec                  
   205,724,063,314      cycles                    #    2.887 GHz                      (49.98%)
    29,614,981,504      instructions              #    0.14  insn per cycle           (62.48%)
     3,443,990,005      branches                  #   48.336 M/sec                    (62.49%)
         4,089,581      branch-misses             #    0.12% of all branches          (62.50%)
    13,976,468,888      L1-dcache-loads           #  196.159 M/sec                    (62.51%)
     3,447,363,194      L1-dcache-load-misses     #   24.67% of all L1-dcache hits    (62.51%)
       162,016,623      LLC-loads                 #    2.274 M/sec                    (50.00%)
        76,612,614      LLC-load-misses           #   47.29% of all LL-cache hits     (49.99%)

      35.763430353 seconds time elapsed

      68.505263000 seconds user
       2.745630000 seconds sys
```

We observe that L1 data cache misses more than double when the thread count is doubled, leading to the negative parallel speed-up. This is a very typical result of running a memory-bound application on more threads.

### Parallel-Friendly Redesign

The random accesses and need for synchronisation severely limited our success in parallelising the sequential method. We need a different algorithmic approach that exposes much more parallelism with better memory access characteristics. This is given in `parallel::sort_first()`. The principal idea is to first sort the edge list by source vertex, `e.u`. It is counter-intuitive, because it increases the serial complexity to `O(m lg m + n)`, but there are four key points to consider:

 * The sort can be parallelised, so it is an overheard that shrinks with increasing cores
 * It leaves the input in the same order as the output, which leads to better, sequential access patterns
 * It allows us to assign ownership of vertices to threads, eliminating the need for synchronisation
 * It is [not the first time](../intro-dm/) that we have seen an `O(n lg n)` algorithm outperform an (expected) `O(n)` algorithm, due to better memory access patterns. Remember that the asymptotic complexity describes superiority _in the limit_, but not necessarily for a specific, fixed range of input values. Given the limited memory and massive parallelism on a GPU, _it is common_ that the cost of an initial parallel sort can be amortised. In fact, the sort itself is often faster than the PCIe transfer!


With the data sorted, we can launch a fully parallel routine that:
  
  1) Finds the number of distinct vertices (`max_vertex`) using a parallel reduction, though this step is followed by a sync barrier so that one thread can allocate space for `node_offsets` (_n_ indexes).
  2) Copies the destination `v_i` from each edge `e_i = ( u_i, v_i )` into the _i_'th index of `neighbours`
  3) Determines for each vertex `u_j` (with binary search) at which index in `edges` is the last edge with a source `u_i < u_j`.

Observe that steps 2 and 3 are independent of each other, too, so we don't need a synchronisation barrier at the end of the second loop (achieved with the `nowait` clause). If a thread finishes its work on step 2, and can move directly onto step 3 before the other threads finish step 2. Removing sync barriers like this generally improves parallel scalability.

#### Profiling/Analysis

This is certainly a different algorithm, and it has worse serial complexity: `O(m lg m + n lg n)` when also taking into account the binary searches on step 3. However, it should have better memory access patterns and better exposes independent data-level parallelism. This first step is to evaluate the overhead in terms of work efficiency, and also the single-threaded efficacy of the better memory access patterns. Running `perf stat -d numactl --physcpubind=0 ./bg`, we observe:

```
answer: 499523
time: 9309.97 us

 Performance counter stats for 'numactl --physcpubind=0 ./bg':

         18,623.30 msec task-clock                #    1.000 CPUs utilized          
                54      context-switches          #    0.003 K/sec                  
                 1      cpu-migrations            #    0.000 K/sec                  
         1,895,318      page-faults               #    0.102 M/sec                  
    55,315,543,991      cycles                    #    2.970 GHz                      (50.00%)
   117,506,753,370      instructions              #    2.12  insn per cycle           (62.50%)
    30,205,351,809      branches                  # 1621.912 M/sec                    (62.50%)
       135,244,762      branch-misses             #    0.45% of all branches          (62.50%)
    29,462,771,666      L1-dcache-loads           # 1582.038 M/sec                    (62.50%)
     1,598,749,272      L1-dcache-load-misses     #    5.43% of all L1-dcache hits    (62.50%)
       352,428,751      LLC-loads                 #   18.924 M/sec                    (50.00%)
       137,975,328      LLC-load-misses           #   39.15% of all LL-cache hits     (50.00%)

      18.624936729 seconds time elapsed

      15.991742000 seconds user
       2.631957000 seconds sys
```

In terms of work efficiency, we have increased the overall number of instructions, relative to `sequential::to_flat_adj_list()` by a factor of `117,506,753,370 / 22,897,619,784 = 5.13Ã—` ðŸ˜¬. At the same time, we have only increased execution time by a factor of `9309.97 / 5312.7 = 1.75Ã—`. The extra instruction count is compensated by the _much_ higher IPC (2.12 insn per cycle compared to 0.74 before).

Indeed, our ratio of L1 d-cache misses has gone done to roughly a quarter of the rate before, reflecting the better access patterns.

Moving to two cores with `sudo perf stat -a --topdown numactl --physcpubind=0,1 ./bg`, we observe:

```
answer: 499523
time: 7923.65 us

 Performance counter stats for 'system wide':

                              retiring      bad speculation       frontend bound        backend bound 
S0-C0           2                31.3%                 4.2%                24.9%                39.6% 
S0-C1           2                26.0%                 2.6%                21.0%                50.3% 

      15.852320504 seconds time elapsed
```

We note two key points from above:

 * The execution time has decreased! I.e., this algorithm is experiencing _positive_ parallel speed-ups
 * We've gone from 7% of instructions retiring in `parallel::to_flat_adj_list()` to 28.5% of instructions retiring now, mostly by converting previously backend-bound instructions.


Finally, we move the experiments back to the 6-core Azure instance to evaluate parallel scalability. Run on a single core, we observe:

```
answer: 499523
time: 7398.14 us
``` 

The single-core performance was quite a bit faster on this machine. Extending to all six cores:

```
answer: 499523
time: 2905.36 us
```

This represents a speed-up of `7398.14 / 2905.36 = 2.55Ã—` on 6 cores. That is not particularly impressive, but it is radically better than what we achieved before. Moreover, this is now `5074.29 / 2905.36 = 1.75Ã—` faster than the original naive sequential baseline. Unlike that baseline, however, this algorithm can be ported to a GPU and the data structure can be processed with a GPU algorithm.

#### Final Notes

In contrast to [../17-work-efficiency](../17-work-efficiency/), this was an example of successfully _sacrificing_ work-efficiency in order to better expose data-level parallelism. To pursue this lecture further (especially if you are using graph algorithms in your project!), it is worth considering the following:

 * **Larger edge lists or more vertices**. The asymptotic cost we paid was in sorting all edges and in conducting a binary search for each vertex. You could try increasing the input parameters _n_ and _m_ to see if these results still hold.

 * **Porting to CUDA**. If you have already gone through [../23-cuda/](../23-cuda/), it would be a good exercise to try to port `parallel::sort_first()` to CUDA as a more complex example

 * **Algorithms over flattened adjacency lists**. We went to some effort to avoid the vector-of-vectors layout. Consider any particular graph algorithm (e.g., breadth-first search) and whether this layout would benefit the performance of that algorithm.
