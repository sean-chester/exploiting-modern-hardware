# Work-Efficiency and Parallel Scalability

## Overview

This lecture explores the concurrent importance of _parallel scalability_ and _work-efficiency_.

**Parallel scalability** refers to the ability to continue gaining performance with the addition of more processors. If `t(i)` denotes the time required for the program when run on _i_ cores, then the _parallel speedup_ provided by _i_ cores is given by:

> t(1)/t(i)

We study parallel scalability by varying _i_ as an independent (_x_-) variable and observing parallel speedup as a dependent (_y_-) variable. Ideal performance is a curve close to `f(x) = x`.

**Work-efficiency** refers to the absence of overhead work for a parallel algorithm. In a theoretical sense, it means that the computational complexity of a parallel algorithm matches that of the best-known sequential algorithm. In a practical sense, it means that the parallel algorithm does more-or-less the same thing as the sequential algorithm, only distributed across multiple cores. We can measure this directly; if `I(i)` denotes the number of instructions retired when the program is run on _i_ cores, and `I(baseline)` denotes the same for the sequential state of the art, then _relative work_ is given by:

> I(i)/I(baseline)

We study work efficiency by again varying _i_ as an independent (_x_-) variable and observing either total work/instructions or relative work. Ideal performance is a constant curve close to `f(x) = 1`. The relative work at `i = 1` implies the minimum number of parallel cores required just to match the sequential state of the art (assuming ideal parallel scalability).

These notions are highly coupled, as it is often easy to obtain parallel scalability by sacrificing work-efficiency and vice versa; algorithms typically trade off one for the other. You can detect this when a research paper reports only parallel scalability and does not compare to the sequential state-of-the-art (perhaps not work-efficient) or does not present experiments with large core counts (perhaps no parallel scalability). You can be confident that you have "solved" a parallel problem when *both* the parallel scalability and work-efficiency are very good.

In this (two-part) live-coding lecture, we will try to design and implement together a work-efficient algorithm with good parallel scalability, following the ideas of [1].


## Problem Definition

Our task for this lesson is to **detect isolated points**. Let `p = (p.x, p.y)` be a two-dimensional point and `d(p,q)` denote the distance between points `p` and `q`. Given a set of points `P` and a range `r`, we want to find the subset `P'` of points that are not within range of any other points; i.e.,

> P' = { p ∈ P: ∄ q ∈ P \ {p}, dist(p,q) ≤ r}.

This is a simplified variant of outlier/anomaly detection and corresponds to a key sub-task for the DBSCAN clustering algorithm [2], i.e., detecting "noise" based on spatial proximity.


## Code Instructions

There are three files in this lesson:
  
  * `point.hpp`: a header-only library that defines a 2d point struct and distance functions thereon
  * `weff.cpp`: the implementation (to be completed) of a work-efficient, parallel anomaly detection algorithm
  * `point-data.hpp`: a static set of randomly generated points, taken uniformly from the positive Euclidean quadrant

To compile the code, you will again need the `-fopenmp` flag to enable OpenMP:

> g++ -Wall -O3 -std=c++17 -mavx -march=native -fopenmp weff.cpp -o weff

There are a number of linux tools that can control the number of cores. In this course, we will use `numactl`. The `--physcpubind` flag specifies which cores should be used by the process. For example:

> numactl --physcpubind=0 ./weff

will run the application on a single core (specifically, the first one), whereas:

> numactl --physcpubind=0,1,2,3 ./weff

will run the application on four cores (specifically the first four).

You can run either `lscpu` or `cat /proc/cpuinfo` to see how many cores are available on your machine.


## Design Sequence

We went through three steps to design this algorithm:

### (Naive) Single-threaded solution

Beginning from just the main method (benchmarking the solution time), we derive a solution to the _Isolated Points_ problem. The naive solution we want to reach has the following features:

 * Triangular nested loops, i.e., `n(n-1)/2` comparisons
 * Logic for maintaining the solution (array of boolean flags)
 * Mechanism for translated the interim data structure (array of flags) into the correct result type, i.e., a `transform_if()` function
 

### Exposing parallelism in naive solution

The second step is to expose substantial data-level parallelism with a fairly even workload balance. The parallel solution has the following features:

 * Substantial data-level parallelism; we assign each outer-loop iteration (value of _i_) as an independent parallel work task
 * Workload balancing (achieved with dynamic rather than static parallelism)
 * Mechanism for handling thread write contention on two-way comparisons

The latter is achieved by recognising that recording a boolean "true" flag is a "safe" sort of write contention (taken from [3]).


### Decreasing work of each thread

On the second day, we analyse the performance of our parallel, naive solution, and see that the IPC is close to 4.0, the L1 cache misses are neglible, and speculation is excellent. Parallel scalability is thus also very good (4.45x speed-up on a 6-core Azure instance). The best opportunity for improvement is in work-efficiency, and we measure the number of instructions.

Some ideas improve work-efficiency, but at the cost of parallel throughput, e.g., introducing a hash set or spatial indexing structure inside our hot inner loop. Instead, we:

 * Pre-sort the data based on manhattan norm (i.e., `p.x + p.y`)
 * Use the property that if `q.x + q.y > p.x + p.y + 2 * r`, then `dist(p, q) > r` to break the inner loop early

This decreases the work of every thread without affecting the amount of parallelism exposed. Therefore, we retain parallel scalability while dramatically improving work-efficiency.

In a raw comparison of the single-threaded naive solution to the parallel naive solution and the work-efficient solution, we see that parallelism alone is not sufficient to compensate poor work-efficiency: the difference was over an order of magnitude. This is true even though there was no asymptotic gain in algorithmic performance: it is still worst-case quadratic.


## Going beyond the lecture

If you want to pursue these concepts further, you could try the following:

 * Increase the dataset size so that it is no longer L1-resident

 * Increase the problem complexity to solve either the skyline problem (as in [1] or [3]) or the DBSCAN clustering problem (as in [2])

 * Investigate how [tiling](../tiling.md) may help performance (subject to first increasing the dataset size)


## Additional Tips

The theme of this lecture was that we need to design algorithms that are both parallel-friendly *and* work-efficient. Here are a few tips for achieving that:

  * Think about exposing wide parallelism early; it can be very difficult to retroactively "add" parallelism

  * Optimise algorithmically for a single instance of work (as a representative for the entire workload). Here we reasoned about what would make it possible to shrink the inner loop the most.

  * Be aware of the sequential state-of-the-art. Without it, you cannot possibly measure work-efficiency correctly.


## References

[1] Chester et al. (2015) "Scalable parallelization of skyline computation for multi-core processors." _Proc. ICDE_. http://sean-chester.github.io/assets/preprints/chester_icde2015_mcsky.pdf

[2] Ester et al. (1996) "A density-based algorithm for discovering clusters in large spatial databases with noise." _Proc. KDD_. https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf

[3] Bøgh et al. (2013) "Efficient GPU-based skyline computation." _Proc. DaMoN_. https://dl.acm.org/doi/10.1145/2485278.2485283
