# Work-Efficiency and Parallel Scalability

## Overview

This lecture explores the concurrent importance of _parallel scalability_ and _work-efficiency_.

_Parallel scalability_ refers to the ability to continue gaining performance with the addition of more processors. If `t(i)` denotes the time required for the program when run on _i_ cores, then the _parallel speedup_ provided by _i_ cores is given by:

> t(1)/t(i)

We study parallel scalability by varying _i_ as an independent (_x_-) variable and observing parallel speedup as a dependent (_y_-) variable. Ideal performance is a curve close to `f(x) = x`.

_Work-efficiency_ refers to the absence of overhead work for a parallel algorithm. In a theoretical sense, it means that the computational complexity of a parallel algorithm matches that of the best-known sequential algorithm. In a practical sense, it means that the parallel algorithm does more-or-less the same thing as the sequential algorithm, only distributed across multiple cores. We can measure this directly; if `I(i)` denotes the number of instructions retired when the program is run on _i_ cores, and `I(baseline)` denotes the same for the sequential state of the art, then _relative work_ is given by:

> I(i)/I(baseline)

We study work efficiency by again varying _i_ as an independent (_x_-) variable and observing either total work/instructions or relative work. Ideal performance is a constant curve close to `f(x) = 1`. The relative work at `i = 1` implies the minimum number of parallel cores required just to match the sequential state of the art (assuming ideal parallel scalability).

These notions are highly coupled, as it is often easy to obtain parallel scalability by sacrificing work-efficiency and vice versa; algorithms typically trade off one for the other. You can detect this when a research paper reports only parallel scalability and does not compare to the sequential state-of-the-art (perhaps not work-efficient) or does not present experiments with large core counts (perhaps no parallel scalability). You can be confident that you have "solved" a parallel problem when *both* the parallel scalability and work-efficiency are very good.

In this (two-part) live-coding lecture, we will try to design and implement together a work-efficient algorithm with good parallel scalability, following the ideas of [1].


## References

[1] Chester et al. (2015) "Scalable parallelization of skyline computation for multi-core processors." _Proc. ICDE_. http://sean-chester.github.io/assets/preprints/chester_icde2015_mcsky.pdf

[2] Ester et al. (1996) "A density-based algorithm for discovering clusters in large spatial databases with noise." _Proc. KDD_. https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf
