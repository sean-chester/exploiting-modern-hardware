/**
 * Toy example to illustrate SIMD/vectorisation.
 *
 * Calculates the average size of set of 3d vectors.
 * Example input: {{1,1,1},{2,2,2}}
 * I.e., 1 vector of size (3*1^2)^-0.5 and 1 vector of size (3*2^2)^0.5
 * Example output: ( sqrt(3) + sqrt(12) ) / 2 = 2.59808
 */


#include "nmmintrin.h" // for SSE4.2
#include "immintrin.h" // for AVX
#include "math.h" 	   // sqrtf()
#include <chrono>      // timing library
#include <cassert>     // assert()
#include <iostream>

#include <vector>

namespace AoS{

// Note that for SIMD, it is *imperative* that data is aligned,
// indicated here by `__attribute__ ((aligned(16)))`; otherwise,
// moving data into SIMD registers will be expensive (if copied
// one-by-one as in __mm_setr_ps()) or a segmentation fault (if
// indicated by a pointer, as in __mm_load_ps()).
// For SSE4.2 (i.e., 128-bit SIMD), 16-byte alignment is required;
// for AVX (i.e., 256-bit SIMD), 32-byte alignment is required.
// In this struct, we assume 128-bit SIMD, because we can only
// make use of up to 3 SIMD lanes, anyway.
struct __attribute__ ((aligned (16))) direction
{
	float x;
	float y;
	float z;
};

#include "3d-vectors.hpp" // statically-generated, 16B-aligned `const direction dirs[]`

namespace nosimd {

/** Compute the average magnitude/length of the global set of 3d direction vectors. */
float average_vector_length()
{
    auto total = 0.0f;
    auto const n = sizeof( dirs ) / sizeof( direction );

    // standard loop; could be parallelised with multicore and/or unrolled for ILP
    // as in previous lectures
    for( auto i = 0lu; i < n; ++i )
    {
        total += sqrtf( dirs[ i ].x * dirs[ i ].x +
                        dirs[ i ].y * dirs[ i ].y +
                        dirs[ i ].z * dirs[ i ].z );
    }

    return total / n;
}
} // namespace nosimd
namespace simd {


/**
 * Compute the average magnitude/length of the global set of 3d direction vectors.
 * "Accelerated" (not very effectively) with SIMD/vectorisation.
 */
float average_vector_length()
{
    auto total = 0.0f;
    auto const n = sizeof( dirs ) / sizeof( direction );

    for( auto i = 0lu; i < n - 1; ++i )
    {
        // To hand-code SIMD, we basically write assembly code.
        // Specifically, we write *intrinsics*.
        // For a complete list, see: https://software.intel.com/sites/landingpage/IntrinsicsGuide/#


        // __m128 is a data type, but refers to a 128-bit register.
        // Contrast this with a float, which is 32-bits. I.e., an __m128
        // is equivalent to a float[4]. (You could treat them the same with
        // a reinterpret_cast<>().)

        // Load 4 floats starting at the given address into a __m128 variable/register
        // Note that the struct is laid out as <x,y,z>, so we start loading from the x-coord.
        // This loads 4!! values, so it will also load the x-value of the next point, too.
        // It is unavoidable, because we need 128 bits to fill the register/data type.
        __m128 const vector4  = _mm_load_ps( &( dirs[ i ].x ) );

        // This is equivalent to vector[4][0]*vector[4][0], ..., vector4[3]*vector[4][3]
        // The result goes into the packed __m128 data type/register, and it is all done in
        // *one* (SSE4) instruction. This provides both parallelism and a lower instruction count
        // You may notice your IPC go down when using SIMD, because your instructions decrease.
        __m128 const squared4 = _mm_mul_ps( vector4, vector4 );

        // Here we cast the __m128 data type in a float[4] so that we can access
        // the values individually. (This technique is generally called "type punning"
        // and shouldn't be done in modern code without a reinterpret_cast<>() to flag it.)
        auto const vec = reinterpret_cast< float const * >( &squared4 );

        // Note, that only 3 of the values were actually useful to us. We ignore the result in vec[3].
        total += sqrtf( vec[ 0 ] + vec[ 1 ] + vec[ 2 ] );
    }

    // The last vector we have to handle separately, because accessing the x-coordinate of the
    // first value after the array would result in undefined behaviour. Probably this is okay,
    // because we aren't using that value anyway; so, running the loop to n-1 and including this
    // non-SIMD'ised final calculation may be overly conservative. But undefined behaviour is
    // generally bad.
    total += sqrtf( dirs[ n - 1 ].x * dirs[ n - 1 ].x +
                    dirs[ n - 1 ].y * dirs[ n - 1 ].y +
                    dirs[ n - 1 ].z * dirs[ n - 1 ].z );

    return total / n;
}

} // namespace simd
} // namespace AoS


namespace SoA {

// This data is identical to the AoS namespace, expect it has been transformed from a vector of 3d structs
// to 3 vectors of floats, each corresponding to the components of distinct vectors. This is analogous to the
// hot/cold decomposition we've seen earlier, but with a very different purpose (exposing more SIMD parallelism).
#include "xvals.hpp"
#include "yvals.hpp"
#include "zvals.hpp"

namespace nosimd {

/**
 * A baseline method using a SIMD-optimised data layout but no SIMD in order to compute the average length of vectors.
 * This implementation is identical to the AoS::nosimd namespace, but uses the different data layout.
 */
float average_vector_length()
{
    assert( "all coords present" && sizeof( xvals ) == sizeof( yvals ) && sizeof( yvals ) == sizeof( zvals ) );

    auto total = 0.0f;
    auto const n = sizeof( xvals ) / sizeof( float );

    for( auto i = 0lu; i < n; ++i )
    {
        total += sqrtf( xvals[ i ] * xvals[ i ] +
                        yvals[ i ] * yvals[ i ] +
                        zvals[ i ] * zvals[ i ] );
    }

    return total / n;
}

} // namespace nosimd

namespace simd {


/**
 * Simd'ised method to compute the average magnitude of the global set of 3d direction vectors.
 *
 * Compared to AoS::simd::, we can chain together all FLOPs with SIMD operations (squaring, summing, and square-rooting).
 * This exposes substantially more SIMD parallelism. It is the data structure redesign that enables it. 
 */
float average_vector_length()
{
    assert( "all coords present" && sizeof( xvals ) == sizeof( yvals ) && sizeof( yvals ) == sizeof( zvals ) );

    // In this case, we can SIMD the *entire* computation; so, even the result is still held in separate lanes.
    // Therefore, we accumulate the results in separate lanes.
    // Note that we also increase to 8-wide rather than 4-wide SIMD, since we are no longer restricted by dimensionality.
    // This command initialises all 8 values to zero.
    auto total = _mm256_setzero_ps();
    auto const n = sizeof( xvals ) / sizeof( float );

    for( auto i = 0lu; i < n; i += 8 )
    {
        // We unroll the loading of x-, y-, and z- components, because we need to operate on
        // them concurrently.
        // Observe that this time we do not need padding, because we can use all 8 values.
        __m256 const x4  = _mm256_load_ps( xvals + i );
        __m256 const y4  = _mm256_load_ps( yvals + i );
        __m256 const z4  = _mm256_load_ps( zvals + i );

        // This is the same as the __mm_mul_ps() in AoS::simd::, except we can apply it 8-wide (__mm256)
        __m256 const xsq4 = _mm256_mul_ps( x4, x4 );
        __m256 const ysq4 = _mm256_mul_ps( y4, y4 );
        __m256 const zsq4 = _mm256_mul_ps( z4, z4 );

        // Because the x-, y-, and z-components are in separate registers, we can also add them together
        // in a vectorised fashion
        __m256 const xy4 = _mm256_add_ps( xsq4, ysq4 );
        __m256 const xyz4 = _mm256_add_ps( xy4, zsq4 );

        // Finally, because the results of the additions are vectorised, we can also square-root the
        // results concurrently (using a unary operation) in all lanes
        __m256 const sqrt4 = _mm256_sqrt_ps( xyz4 );

        // Finally, since our results are stored in lanes, we can SIMD-add our results to the running total
        total = _mm256_add_ps( total, sqrt4 );
    }

    // In the end, we need to reinterpret as a float[ 8 ], as in AoS::simd::, but we have deferred
    // this until outside the loop, rather than on every loop iterations.
    auto const mag4 = reinterpret_cast< float const * >( &total );
    return ( mag4[ 0 ] + mag4[ 1 ] + mag4[ 2 ] + mag4[ 3 ] 
           + mag4[ 4 ] + mag4[ 5 ] + mag4[ 6 ] + mag4[ 7 ] )
           / n;
}
} // namespace simd
} // namespace SoA



int main()
{
    auto sum = 0.0;
    auto const num_trials = 20000u;

    auto const start_time = std::chrono::system_clock::now();

    for( auto i = 0u; i < num_trials; ++i )
    {
        // Toggle namespaces to get nosimd:: or simd:: version
        sum += AoS::simd::average_vector_length();
        // sum += AoS::nosimd::average_vector_length();
        // sum += SoA::simd::average_vector_length();
        // sum += SoA::nosimd::average_vector_length();
    }

    auto const end_time = std::chrono::system_clock::now();
    auto const elapsed_time = std::chrono::duration_cast<std::chrono::microseconds>( end_time - start_time );

    std::cout << "answer: " << ( sum  / static_cast< float >( num_trials ) ) << std::endl;
    std::cout << "time: " << ( elapsed_time.count() / static_cast< float >( num_trials ) ) << " us" << std::endl;
    return 0;
}
