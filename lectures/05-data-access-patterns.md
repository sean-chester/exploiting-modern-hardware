# 05 - Data Access Patterns

This lecture introduces the importance of data access patterns and memory layout, by means of discussing:
> Adam Jacobs (2009). "The Pathologies of Big Data." _ACM Queue_ 7(6).

## Lecture Notes

The following is a loose transcription of the main points addressed during the lecture.

### Purpose of the paper

What are the main take-aways from this article?

 * Question the use of relational data models for Big Data processing (second column, page 5)
   + The relational model (as we learn in courses like CSC 570) is set-based, but most real data has some notion of order --- typically temporal.
   + Efficient data processing must take traversal order into account

 * Defining "Big Data" (middle column, page 9)
   + Temporally dependent: stretches limits of contemporary tools, be that Excel in the 90's or R and postgres at the time of writing
   + Always (has been) relevant: we are always trying to stretch limits

 * Distributed computing as a solution (page 8)
   + Visionary at the time, pre-cloud
   + Not necessarily necessary today (with multicore, gpu's)

Paper metadata (reliability of source)
 * [ACM Queue](https://queue.acm.org/author_guidelines.cfm) = inivitation-only ACM magazine targetting tech leads, senior software devs
 * Adam Jacobs = senior software engineer for Tenbase (high-performance analytical database), bio at end of article

Important terminology
 - cycle = one tick of the processor's internal clock
 - stalls = cycle in which an instruction cannot be retired
 - cache miss = load request when something is not available in cache
 - cache stall = cache miss that coincides with a stall
 - memory-bound = application cannot go faster because it is waiting for memory operations (loads and stores)
 - compute-bound = application cannot go faster because it cannot retire any more instructions per cycle
 - xyz-bound = application cannot go faster because xyz is the limiting factor (e.g., cache-, bandwidth-, IO-)
 - retired instruction = instruction that has been completed and does not need to be ro

Basic improvement strategy
  - if memory-bound, reduce memory stalls
    * increase cache size (if you are a processor manufacturer!)
    * improve cache utilisation (locality, see later)
    * decrease working set size (c.f., Drepper paper)

  - if compute-bound
    * use parallism (especially shared memory, where transfer costs do not necessarily impact compute-boundedness)
    * reduce instructions by optimising code, micro-optimisations
    * choose another algorithm that requires less overall "work" (# instructions)

Figure 3, what is critical to observe?
  - sequential access is always faster than random access on the same device (memory, disk, or SSD/flash)
    * relates to [seek time](https://en.wikipedia.org/wiki/Hard_disk_drive_performance_characteristics#Access_time): for a random access, the disk needle has to seek the correct position on the disk to read a value. For multiple random accesses, multiple seek times are incurred. For multiple sequential accesses, only the first read incurs a seek.
  - memory is always faster than disk for the same access pattern
    * We will discuss memory latency and memory hierarchies more when we discuss Drepper (2007); put simply, disk is farther away and lower bandwidth than memory.  
  - implications for I/O model (similar to for RAM model in previous lecture)
    * random RAM is slower than sequential disk. Mainstream database thought is that disk is so much slower than RAM that we can ignore RAM operations!
    * Our theoretical models are tremendously useful, but we also need to bear in mind their fundamental assumptions and when those do/n't apply

Locality (of reference)
  - Accessing things close to each other
  - Temporal locality = accessing same thing recently, e.g., reusing values shortly after just read them
    * already available in cache or register; opposite = cache eviction because it's been so long at it's not _least recently used_
  - Spatial locality = accessing something physically close to a recent access, e.g., sequential access
    * benefits prefetching, doesn't require much seek time
  - Figure 1 shows a "packing" of data without any waste of bits
    * decreases overall memory consumption
    * increases temporal locality because more values fit on same (typically 64-byte) atomic read/write unit, a.k.a., "cache line"
      + accessing any values on the same cache line immediately after each other doesn't require reading a new cache line from memory
  - Figure 4 shows ["denormalisation"](https://en.wikipedia.org/wiki/Denormalization) of a different (join) query
    * increases overall memory consumption
    * but also increases spatial locality: alternative join requires substantial random accesses to find missing pairs (when tables not sorted on join key)
  - These techniques effectively improve locality, but everything comes at a cost
    * packing requires "unpacking" (c.f., Figure 1 pseudocode) to transform the packed values back into the normal 32-bit values that can be stored in a register; i.e., we trade-off cache/memory performance for more instructions
      + illustrative of why it is important to know whether we are compute-bound or memory-bound when trying to optimise algorithms, data structures, or implementations
    * both come at a cost of readability and maintenance (Software Engineering perspective)

## Notes not covered in lecture

 - transaction vs olap/warehouse processing, in vs out, data analytics, management, science on latter
   + "“What happened?”   “Why   did   it   happen?” “What’s going to happen next?”
 - postgres switches algorithmic strategies based on size of data
   + why would it use hash for small data and sorts for large data?
     * what does Jacobs think of that?
   + "whereas  it  is  typically mitigated by various kinds of caching when  data  sizes  are  small"
     * can we make "Big Data" behave like small data?

## Coming lecture(s)
  - Discuss classic [Array of structs vs struct of arrays](https://en.wikipedia.org/wiki/AoS_and_SoA) design decision (extension of packing vs denormalisation): often the best, first idea to improve memory-bound applications/data structures
  - Investigate Intel top-down methodology for diagnosing bottlenecks
    * Learn about hardware performance counters and a library for measuring them