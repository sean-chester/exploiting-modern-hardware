# Synchronisation and Atomics

## Overview

In this class, we are following a [parallel-for model](https://software.intel.com/en-us/videos/part-5-parallel-loops-private-and-shared-variables-scheduling) of parallelism, where we structure our computation into a wide set of independent tasks that can be assigned (transparently) to separate threads of execution. (This process could be repeated in multiple stages.) This is a sub-case of the more general _fork-join_ model that is [supported natively by C++](https://en.cppreference.com/w/cpp/thread/thread/join).  Sometimes, however, concurrent threads need to _coordinate_ around shared resources. Both the joining of threads at the end of the loop and the coordination of shared resources are referred to as _synchronisation_. Generally speaking, synchronisation inhibits parallelism, as it requires otherwise independent threads to wait for each other. This lesson focuses on coordinating shared resources.

Concurrent threads can safely read shared data at the same time, but data races may occur whenever at least one thread modifies the data. This leads to incorrect results (as demonstrated in the first part of this live-coding session). For correctness, we need to ensure _atomicity_ of the operations (sometimes referred to as a ["transactional memory model"](https://en.cppreference.com/w/cpp/language/transactional_memory), much like the notion of database transactions). An atomic block is observed by another thread as either having occured yet or not occurred yet, but cannot be partially complete.

This is often achieved with _locking_ [1]. A _mutual exclusion_ (mutex) lock is connected to a resource (e.g., memory address) and needs to be _acquired_ by a thread before that thread can access the connected resource. When the thread is finished with the resource, it _releases_ the lock so that other threads can acquire it. Naturally, this serialises parallel code, as each thread must take a turn accessing the shared resource. But what happens if two threads want to acquire the same lock _at the exact same time_? Does the lock itself require a lock? It seems like [turtles all the way down](https://en.wikipedia.org/wiki/Turtles_all_the_way_down)!

Locking is often achieved with very lightweight _hardware locks_, known as _atomic instructions_. We can implement the acquiring of a lock via an atomic _compare and swap_ instruction. Thus, rather than focus on mutexes, today we will focus on using atomic instructions to synchronise shared resources. You can then imagine scenarios in which atomics can be directly used, you could implement your own mutex, or you can simply have a better conceptual understanding of how synchronisation can be done.

In this live-coding lecture, we will work through a basic example of concurrent writes to an otherwise _not thread-safe_ container using the atomic _fetch and add_ instruction. We will study how the user input affects the efficacy of this approach, which is vulnerable to _false sharing_.


## Problem Definition

Today we again tackle the _Sum Over Threshold_ problem, as in [the previous lecture](../15-openmp/). Specifically,
we are given a vector of numbers, `v = < v_i >` and a threshold `t`, and we want to calculate the sum over all elements in `v` that exceed the threshold:

```
SOT( v, t ) = {v_i âˆˆ v, v_i â‰¥ t}
```

However, this time we want to report the list of values matching the predicate, not just their sum. Moreover, we want to investigate sychronisation-based solutions rather than applying a parallel reduction.


## Code Instructions

There are two files in this lesson:
  
  * `lockfree.cpp`: the implementation of two synchronisation-based approaches
  * `../15-openmp/unsorted-data.hpp`: an unsorted vector of unsigned integers

To compile the code, you will need the `-fopenmp` linker flag to enable the OpenMP multicore parallelisation library:

> g++ -Wall -O3 -std=c++17 -fopenmp -mavx -march=native lockfree.cpp -o lf


## Lesson Sequence

### (Incorrect) Non-sychronised Solution

As in the [parallel reduction lecture](../15-openmp/), we begin with an efficent but branching solution that works on a single thread. This is the solution recorded in the namespace `nosync::`. It is not _thread-safe_; i.e., when multiple threads collaborate, they create a data race on the output vector. `std::vector<>::push_back()` is not thread-safe, so we implement our own version `std::vector<>` by pre-allocating the entire array and then keeping track of its size in a variable `result_size`. Our solution is still not thread-safe, however, because threads may content to change `result_size` at the same time.

#### Profiling/Analysis

In this lesson, we profile in terms of parallel speed-up and correctness. To simplify measuring correctness to just evaluating the size of the result vector. When run on one core, e.g.:

> numactl --physcpubind=0 ./lf 500

We observe output similar to:

```
answer: 1.50094e+06
time: 14481.6 us
```

When increasing this to two threads:

> numactl --physcpubind=0,1 ./lf 500

We observe output similar to:

```
answer: 750445
time: 7863.6 us
```

We see a very impressive speed-up of `14481.6/7863.6 = 1.84Ã—`. However, the answer is very much _incorrect_, owing to the data race. 


### Synchronisation with Atomics

Our second approach, in namespace `atomics::`, uses _hardware atomics_ for lightweight synchronisation. Specifically, we use the _atomic fetch and add_ instruction in order to atomically:

  * determine the current value at an address
  * increment that value by a specific amount

In this case, we use the atomic on the size of our output array, `result_size`, atomically incrementing it by 1. When a thread executes this instruction, it effectively increases the size of the array by one and reserves for itself the "tail" location of the array. Since these increments are atomic, each thread will always get a unique write location. At an implementation level, we achieve this with the `gcc` _intrinsics_, i.e., special, compiler-specific functions that directly invoke a particular processor instruction. For _fetch and add_, the intrinsic is `__sync_fetch_and_add( addr, val )`.


#### Profiling/Analysis

We begin again by measuring single-core performance. This gives us an indication of the overhead introduced by the atomic instruction. We observe results similar to:

```
answer: 1.50094e+06
time: 20064.8 us
```

This indicates a fairly steep overhead of `(20064.8-14481.6)/14481.6 = 39%`. Moreover, when we parallelise to two cores, we observe results similar to:

```
answer: 1.50094e+06
time: 59439.7 us
```

The great news is that our parallel program is now correct! The unfortunate news is that it obtains a speed-up of `20064.8/59439.7 = 0.34Ã—` ðŸ˜¬. This should be surprising, because the writes to the output vector are all fully independent, but there are two explanations: 

  * Even when we use a lightweight hardware lock, the cost of synchronisation in this case makes parallelism detrimental!
  * There is _false sharing_: even though the threads write to separate values in the array, they are writing to the same cache lines! This causes a significant slowdown in order to maintain cache coherence.

However, let us try again with a new threshold, e.g.:

> numactl --physcpubind=0 ./lf 999

We observe much faster results, because the branch is more predictable (almost always false) and we do far fewer writes:

```
answer: 2955
time: 3066.69 us
```

Scaling to two cores:

> numactl --physcpubind=0,1 ./lf 999

```
answer: 2955
time: 2020.78 us
```

We observe this time a speed-up of `3066.69/2020.78 = 1.52Ã—`, which is nearly as good as our naive, incorrect solution earlier! If you re-run namespace `nosync::` on this input, you will observe two-core performance close to 1956.85 us. With far fewer writes overall, the probablity of both threads trying to write to `result_size` at the same time is very low, so they don't block each other. This reveals the main consideration when using locking/sychronisation as a model to manage a lack of exposed parallelism. The success depends greatly on the probability of threads colliding on the lock simultaneously: lightweight locks can be very effective if the odds are low that they are needed; if they are clearly needed, then your code may experience negative speed-ups in parallel as the locks serialise the threads!


### OMP Critical Sections

The final approach is given in namespace `critical::`. OpenMP makes it easy to lock an entire section of code in a generic fashion. This mimics the performance of the intrinsics in namespace `atomics::`. However, note that only one thread can enter that code block at once, even if they actually want to write to separate addresses. The performance is the same because we only have one address (`result_size`) with a data race. In general, it may be that we try to increment different counters at different locations; then the lightweight lock of a single address is much finer-granularity synchronisation than locking entry into an entire code block. In general, we want the granularity of synchronisation to be as fine as possible.


## Final Notes

If you are interested in pursuing this topic further, I suggest the following topics:

 * **Try out other atomic operations**. The other really common atomic modify-and-write operation is an _atomic compare and swap_. Read about other atomic operations from [Intel](https://software.intel.com/en-us/node/506090) or the [C++ Atomic template class](https://en.cppreference.com/w/cpp/atomic/atomic).

 * **Play with the compute-to-write ratio**. This lesson featured a problem with very little compute-work, so the rate at which we tried to write to the shared variable (`result_size`) was very high. Typically, one might do a lot more processing before deciding to write a value to an output stream. This can shift the probability of threads colliding, as they spend less time overall on the locked address.

 * **Try to complete a thread-safe producer-consumer queue**. We designed a write-only queue with multiple producers. You could consider how other threads may try to consume data from the front of the queue while it is being populated. See [this Dr. Dobb's article](https://www.drdobbs.com/parallel/writing-lock-free-code-a-corrected-queue/210604448) for some ideas of how to achieve that.

However, mostly in this class we will try to redesign algorithms to expose more parallelism, thereby avoiding these locks in the first place; i.e., we want to drive the probability of concurrent access down to 0.00.

## References

 [1] Dijkstra (1965). "Solution of a problem in concurrent programming control." _Comm ACM_ 8(9). http://ezproxy.library.uvic.ca/login?url=https://dl.acm.org/doi/10.1145/365559.365617 