/**
 * A demonstration of multi-threaded synchronisation via hardware atomics.
 * We are building an append-only "lock-free" queue (i.e., one half of a
 * producer/consumer queue).
 *
 * Demo problem is to return a sub-vector of all numbers >= 500, using parallelism
 * E.g., {428, 827, 2, 111, 949}
 * returns {827, 949}.
 */

#include <iostream>  // std::cout, std::endl;
#include <chrono>    // timing libraries
#include <omp.h>     // OpenMP parallelisation library

#include <vector>

namespace { // anonymous

#include "../15-openmp/unsorted-data.hpp" // statically-generated input data

} // namespace anonymous
namespace nosync {

template < typename T >
    std::vector< T > over_threshold( T const threshold )
    {
        auto const n = sizeof( input_data ) / sizeof( input_data[ 0 ] );

        // Note that std::vector<>::push_back() is not thread-safe! I.e.,
        // calling that method from multiple threads results in *undefined behaviour*.
        // Instead, we allocate an array up-front and _manually_ manage the
        // "push_back()" functionality: we keep track of the tail of the array
        // and insert new elements in that index, then increment the index.
        std::vector< T > result( n );

        auto result_size = 0lu;

        #pragma omp parallel for
        for( auto i = 0lu; i < n; ++i )
        {
            if( input_data[ i ] >= threshold )
            {
                // Here we insert the new value and increment the result size, but it's
                // also not thread-safe! Multiple threads could read `result_size` at the
                // exact same time and then all write to the same location, overwriting
                // each other.
                // This method *does not* produce correct results when run with multiple threads!
                result [ result_size++ ] = input_data[ i ];
            }
        }

        result.resize( result_size );
        return result;
    }

} // namespace nosync
namespace atomics {

template < typename T >
    std::vector< T > over_threshold( T const threshold )
    {
    	auto const n = sizeof( input_data ) / sizeof( input_data[ 0 ] );
    	std::vector< T > result( n );

    	auto result_size = 0lu;

    	#pragma omp parallel for
    	for( auto i = 0lu; i < n; ++i )
    	{
    		if( input_data[ i ] >= threshold )
    		{
                // This time we synchronise with an atomic instruction.
                // __sync_fetch_and_add() is a compiler intrinsic that 
                // invokes a lightweight hardware-lock.
                // It adds the second argument to the address given by the
                // first argument. The return value is the value that was
                // in the memory address prior to the increment.
                //
                // In our context, this gives us the current size of the
                // result array in variable tail, freezes the address,
                // and increments it by 1.
    			auto const tail = __sync_fetch_and_add( &result_size, 1 );

                // Now our thread has a unique location in which to write its
                // result. No other thread will try to write to this location;
                // so, we solved the data race from the nosyncs:: approach.
                // However, as mentioned in the notes, now we have *false sharing*. ☹️
    			result [ tail ] = input_data[ i ];
    		}
    	}

    	result.resize( result_size );
    	return result;
    }

} // namespace atomics
namespace critical {

template < typename T >
    std::vector< T > over_threshold( T const threshold )
    {
        auto const n = sizeof( input_data ) / sizeof( input_data[ 0 ] );
        std::vector< T > result( n );

        auto result_size = 0lu;

        #pragma omp parallel for
        for( auto i = 0lu; i < n; ++i )
        {
            if( input_data[ i ] >= threshold )
            {
                // There are more portable approaches to the compiler intrinsics.
                // OpenMP has a "critical" pragma that only allows one thread to enter.
                // (C++17 also has a std::atomic<>() template type for POD).
                // This isn't quite the same as the atomics:: solution though,
                // as no two threads can execute this code at the same time,
                // even if they want different variables (i.e., not both `result[ result_size ]`).
                // The atomics:: on the other hand will not block each other unless they
                // try to access the exact same memory address.
                #pragma omp critical
                result [ result_size++ ] = input_data[ i ];
            }
        }

        result.resize( result_size );
        return result;
    }


} // namespace critical

int main( int argc, char **argv )
{
    if( argc < 2 )
    {
        std::cout << "Usage: " << argv[ 0 ] << " [threshold]" << std::endl;
        return 0;
    }

    auto sum = 0lu;
    auto const threshold = static_cast< uint32_t >( atoi( argv[ 1 ] ) );
    auto const num_trials = 500lu;

    auto const start_time = std::chrono::system_clock::now();

    for( auto i = 0u; i < num_trials; ++i )
    {
        // Toggle between the lesson options by changing the namespace below:
        sum += nosync  ::over_threshold( threshold ).size();
        // sum += atomics ::over_threshold( threshold ).size();
        // sum += critical::over_threshold( threshold ).size();
    }

    auto const end_time = std::chrono::system_clock::now();
    auto const elapsed_time = std::chrono::duration_cast<std::chrono::microseconds>( end_time - start_time );

    std::cout << "answer: " << ( sum  / static_cast< float >( num_trials ) ) << std::endl;
    std::cout << "time: " << ( elapsed_time.count() / static_cast< float >( num_trials ) ) << " us" << std::endl;
    return 0;
}