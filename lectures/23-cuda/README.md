# Introduction to CUDA

## Overview




## Problem Definition

Today we are not solving a new problem, just porting an existing solution from gcc intrinsics for SIMD to CUDA for the GPU. We reuse the problem from the [SIMD lecture](../19-simd/): to **calculate average vector length**. Let `v = (v.x, v.y, v.z)` be a three-dimensional vector and `|v|` denote the length (i.e., magnitude) of `v`, i.e., `sqrt(v.x^2 + v.y^2 +v.z^2)`. Given a set of vectors `V`, we want to compute the average length:

> Σ_{v∈V}|v|/|V|.

This time we want to solve it in the SIMT model, which is more general than SIMD.


## Code Instructions

There are five files in this lesson:
  
  * [cuda-intro.cu](cuda-intro.cu): the CUDA implementation of our average vector length calculation. Note the `.cu` extension for CUDA.
  * [../19-simd/simd.cpp](../19-simd/simd.cpp): the SIMD/CPU baseline to which we will compare the CUDA implementation
  * [../19-simd/xvals.hpp](../19-simd/xvals.hpp): a flat 1d array of _n_ x-coordinates
  * [../19-simd/yvals.hpp](../19-simd/yvals.hpp): a flat 1d array of _n_ y-coordinates
  * [../19-simd/zvals.hpp](../19-simd/zvals.hpp): a flat 1d array of _n_ z-coordinates

To compile the code, you will need CUDA and nvidia drivers installed, as well as the `nvcc` compiler (available in the `nvidia-cuda-toolkit` package on Ubuntu). To set this up at home (if you have a dedicated Tesla GPU for the purpose), you can follow [these instructions](https://askubuntu.com/a/1036265/747386). Alternatively, you can work through this lesson on the Azure instance, where the environment is already set up to make use of the [nvidia Tesla V100 GPU](https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf).

You can compile the CUDA code as follows:

> nvcc -O3 cuda-intro.cu

However, the `nvcc` compiler will not compile the SIMD intrinsics used in `../19-simd/simd.cpp`, so for that you will still need to use `gcc`:

> g++ -Wall -O3 -std=c++17 -mavx -march=native simd.cpp -o simd


## Lesson Sequence

### Establishing Baseline Performance/Correctness

Debugging parallel code is difficult in general. Debugging GPU code can be downright challenging, as you don't have access to a debugger or to `std::cout` statements. Thus, it is very important to have some black-box or unit tests in place. We will check both correctness and performance relative to the baseline that we already established in [the SIMD lecture](../19-simd/). It is not critical which of the four namespaces/solutions from that exercise that you use, as they are all correct.

> ./../19-simd/simd

You should observe results similar to:

```
answer: 962.58
time: 13.4074 us
```

We will use these numbers as baselines for analysing the CUDA implementation.


### Port to CUDA

[cuda-intro.cu](cuda-intro.cu) gives the same code ported onto the GPU. Note that we have to manage the memory transfer ourselves, as the CPU (host) and GPU (device) have separate memory. So, prior to calling the GPU "kernel" (i.e., function), we have to allocate memory and transfer the data there. Afterwards, we have to transfer the results back and free the memory. Most of the additional CUDA functions in the code are managing these memory transfers and (de-)allocations.

The kernel itself (function `vector_length()`) looks like regular C, but carries the `__global__` keyword. It specifies the code that a single thread should do. It is very much like the syntax used in [C++ threads](http://www.cplusplus.com/reference/thread/thread/). The kernel is executed by each thread; so, it begins with logic for the thread to identify itself. This establishes a one-to-one mapping between threads and data elements.

If you run the code, you should observe results similar to:

```
answer: 962.58
time: 365.133 us
```

Observe that the result matches our baseline (correctness), but it is substantially slower (performance)! You should pause for a few minutes here to contemplate why that might be.

#### Profiling/Analysis

We can run the built-in nvidia profiler to better understand why the GPU implementation is so much slower:

> nvprof ./cuda

You should observe results similar to:

```
==76673== NVPROF is profiling process 76673, command: ./cuda
answer: 962.58
time: 358.531 us
==76673== Profiling application: ./cuda
==76673== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   75.71%  355.44ms     60000  5.9230us  5.8550us  6.3360us  [CUDA memcpy HtoD]
                   16.90%  79.318ms     20000  3.9650us  3.8710us  4.4480us  [CUDA memcpy DtoH]
                    7.39%  34.686ms     20000  1.7340us  1.6950us  3.9680us  _GLOBAL__N__45_tmpxft_0000de3e_00000000_8_cuda_intro_cpp1_ii_main::vector_length(float*, float*, float*, float*, unsigned long)
      API calls:   43.19%  2.91278s     80000  36.409us  4.0000us  379.51ms  cudaMalloc
                   29.36%  1.97998s     80000  24.749us  3.0000us  1.4723ms  cudaFree
                   23.29%  1.57112s     80000  19.639us  9.3990us  57.010ms  cudaMemcpy
                    3.77%  254.18ms     20000  12.709us  10.799us  1.5435ms  cudaLaunch
                    0.25%  16.566ms    100000     165ns      99ns  210.09us  cudaSetupArgument
                    0.13%  9.0674ms     20000     453ns     299ns  33.598us  cudaConfigureCall
                    0.01%  594.77us         1  594.77us  594.77us  594.77us  cuDeviceTotalMem
                    0.01%  530.28us        94  5.6410us     100ns  389.58us  cuDeviceGetAttribute
                    0.00%  16.000us         1  16.000us  16.000us  16.000us  cuDeviceGetName
                    0.00%  1.7000us         3     566ns     100ns  1.1000us  cuDeviceGetCount
                    0.00%     800ns         2     400ns     200ns     600ns  cuDeviceGet
```

This gives a breakdown of the time spent in each function, both in raw time (milliseconds) and percentage of overall run time. It also shows the number of times that each function was called and some basic statistics (min, max, average) on time per call. Recall that we wrapped our benchmark in 20000 runs; so `vector_length`, for example, is called 20000 times.

Observe that, on average, our CUDA kernel actually executes in just 1.734 microseconds! So, although it doesn't include the reduction to calculate the average, it is still about `13.4074/1.734 = 7.7×` faster than the SIMD version; i.e., the GPU _is_ much faster.

The catch is that we make 4 transfers back and forth to the device, and these transfers together account for 93% of the GPU time. Looking at the API calls, we also see that the allocations and de-allocations were very expensive. It might make sense not to include these in our benchmark, depending on context, but the transfers to/from the device should (almost) always be taken into account when comparing to a CPU-only algorithm if the comparison is to be fair.

If we look only at the time spent copying data _to the device_, it cost on average `3 * 5.923 = 17.7` microseconds, i.e., already about 25% more time than the CPU solution, before we have even launched the kernel!

The key take-away from this lesson is that it can be hard to amortise that fixed cost of transferring data. In this case the amount of work done by each thread (calculate the magnitude of a vector) was too small. While we want to expose wide, fine-grained parallelism, we also need to ensure that there is enough work to justify the cost of the transfer. No GPU algorithm could possibly outperform our CPU algorithm in this case, because the CPU has already finished before the GPU even begins!


### Re-implementation

Having gone through the inline documentation in `cuda-intro.cu` and this analysis above, you should try to report [the SIMD code](../19-simd/simd.cpp) into CUDA yourself by hand, referring back to `cuda-intro.cu` as seldomly as possible. This will help you to better understand how to manage the memory and invoke GPU kernels.


## Final Notes

If you are interested in pushing this lesson further, I suggest the following ideas:

 * **Vary the problem to increase compute work**. The limitation here is our problem; it is simply too fast on the CPU to justify the GPU. Try to come up with a slight variation on the problem that dramatically increases the amount of computation done by each thread and see if you can amortise the cost of the transfers.

 * **Try to decrease the size of the DeviceToHost transfer**. In this lesson we only used the GPU to calculate the magnitudes of the vectors; the average of the magnitudes was calculated on the CPU. This is often called _GPU-CPU co-processing_, where both the GPU and the CPU are involved in different parts of the computation. This approach led to transferring back _n_ floats. In a subsequent lesson we will try to reduce this _on the GPU_. See if you can anticipate that lesson by implementing a reduction on the GPU and profile the results to see if you obtain a performance improvement. (Hint: it will require more than one kernel.)
