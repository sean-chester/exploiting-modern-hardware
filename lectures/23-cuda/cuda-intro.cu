/**
 * Toy example porting a simd algorithm to CUDA for an NVIDIA GPU
 *
 * Calculates the average size of set of 3d vectors.
 * Example input: {{1,1,1},{2,2,2}}
 * I.e., 1 vector of size (3*1^2)^-0.5 and 1 vector of size (3*2^2)^0.5
 * Example output: ( sqrt(3) + sqrt(12) ) / 2 = 2.59808
 */

#include "math.h" 	   // sqrtf()
#include <chrono>      // timing library
#include <cassert>     // assert()
#include <numeric>     // std::accumulate()
#include <iostream>


namespace { // anonymous

// This global constant denotes the number of threads in each thread block
// In principle it could be any number, but it doesn't generally make sense
// to use numbers that are not multiples of 32 (the fixed number of threads
// per warp). The max number of threads per thread block is 2048.
// In other words, this is a tunable parameter.
int const blocksize = 512; 

// Reuse statically generated data from previous lecture
#include "../19-simd/xvals.hpp"
#include "../19-simd/yvals.hpp"
#include "../19-simd/zvals.hpp"


// This is a CUDA device kernel (a.k.a., function). It runs on the GPU.
// The `__global__` keyword identifies it as a device kernel.
// Observe the thread-centric view: each thread will execute this function
// independently. Because it is a device kernel, the pointers passed as arguments
// must be allocated *on the device.* The input size, n, however, is passed by value,
// not by pointer, so a separate copy is created and it can reflect a host variable.
__global__
void vector_length( float *x, float *y, float *z, float *result, size_t n )
{
    // Here we calculate the global id of the thread. In our example, each block
    // can have `blocksize = 512` threads, so `threadIdx` ranges from [0, 512), i.e.,
    // is local to the block. We need to calculate the offset from the blockIdx in
    // order to expand to the range [0, n).
    // Note that `threadIdx`, `blockIdx`, and `blockDim` are all built-in variables in CUDA.
    int const index = threadIdx.x + blockIdx.x * blockDim.x;

    // The number of threads that we launch is a multiple of the blocksize, but the number
    // of data points isn't necessarily. This condition idles all threads (which are only
    // found in the final thread block) whose id exceeds the data range. This avoids reading
    // off the end of the arrays. Note that although we idle these threads, we didn't have any
    // more work to assign them anyway.
    if( index < n )
    {
        // Much CUDA kernel code is just straight-forward C.
        // Here, this thread computes the result (magnitude) for its individually-assigned vector.
        result[ index ] = sqrt( x[ index ] * x[ index ]
                              + y[ index ] * y[ index ]
                              + z[ index ] * z[ index ] );
    }
}

// This is a normal CPU host function, but it contains built-in CUDA functions to initialise the GPU kernel.
// It returns the average length of all the input vectors, exactly as in lecture 19-simd. 
float average_vector_length()
{
    assert( "all coords present" && sizeof( xvals ) == sizeof( yvals ) && sizeof( yvals ) == sizeof( zvals ) );
    auto const size = sizeof( xvals );
    auto const n = size / sizeof( xvals [ 0 ] );

    // Note that we fixed the number of threads per thread block to be blocksize.
    // This expression calculates how many thread blocks we would need in order to have one thread for
    // every input vector, i.e., the minimum number of thread blocks to ensure we have at least n threads.
    auto const num_blocks = ceil( n / static_cast< float >( blocksize ) );

    // These are (uninitialised) pointers to memory on the GPU. It is standard to prefix device pointers
    // with a `d_` or something similar. (I've used the `dev_` prefix to denote a device pointer.)
    float *dev_x, *dev_y, *dev_z, *dev_result;

    // We need an output array in which to transfer back the results from the GPU/device. 
    float result[ n ];

    // Each of these allocations memory on the GPU for our input (first three) and output (last one).
    // Observe that we bind the allocated memory to the device pointers that we declared above.
    cudaMalloc( (void **) &dev_x, size );
    cudaMalloc( (void **) &dev_y, size );
    cudaMalloc( (void **) &dev_z, size );
    cudaMalloc( (void **) &dev_result, size );

    // Each of these functions initiates a transfer of data between the host (CPU) and device (GPU).
    // The syntax is `(destination,source,size,direction)`.
    // Note that the `cudaMemcpyHostToDevice` constant denotes transferring data *to the GPU*.
    cudaMemcpy( dev_x, xvals, size, cudaMemcpyHostToDevice );
    cudaMemcpy( dev_y, yvals, size, cudaMemcpyHostToDevice );
    cudaMemcpy( dev_z, zvals, size, cudaMemcpyHostToDevice );

    // At last, we invoke the code on the GPU, using the data that we just transferred there.
    // It looks like normal C++ template code, except that the special syntax `<<<x,y>>>` configures
    // the assignment of threads to thread blocks. This is going to launch `num_blocks * blocksize` threads.
    // All threads in the same thread block have mutual access to shared memory.
    vector_length<<< num_blocks, blocksize >>>( dev_x, dev_y, dev_z, dev_result, n );

    // Once the kernel has completed, we initiate a transfer of the result data *back to the CPU*.
    // Note that the `cudaMemcpyDeviceToHost` constant denotes transferring data *from the GPU*.
    cudaMemcpy( result, dev_result, size, cudaMemcpyDeviceToHost );

    // Finally, because we are using old-fashioned mallocs, we need to manually clean-up after ourselves
    // These functions free memory that was allocated on the GPU/device.
    cudaFree( dev_x );
    cudaFree( dev_y );
    cudaFree( dev_z );
    cudaFree( dev_result );

    // Compute the average of all the magnitudes
    return std::accumulate( result, result + n, 0.0f ) / n;
}

} // namespace anonymous


int main()
{
    auto sum = 0.0;
    auto const num_trials = 20000u;

    auto const start_time = std::chrono::system_clock::now();

    for( auto i = 0u; i < num_trials; ++i )
    {
        sum += average_vector_length();
    }

    auto const end_time = std::chrono::system_clock::now();
    auto const elapsed_time = std::chrono::duration_cast<std::chrono::microseconds>( end_time - start_time );

    std::cout << "answer: " << ( sum  / static_cast< float >( num_trials ) ) << std::endl;
    std::cout << "time: " << ( elapsed_time.count() / static_cast< float >( num_trials ) ) << " us" << std::endl;
    return 0;
}
