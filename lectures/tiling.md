# Improving locality by design
 
 * Review of Drepper paper, class walk-through of critical points.
   + Key linking piece, be sure to understand this well before next week's lectures
   + Too much content to repeat experiments in class, making up for snow day, so we'll take his #s for granted
 * Another resource to come back to throughout your career, rich and layered
   + Chapter 2 is fascinating. I'm not a ceng, but it answers the question of why the world is the way it is :)
 * Figure 3.2 (pg 15): fine conceptual model of cache
   + Intel = inclusive, i.e., everything in L1 is in L2, ...
   + Also shows instruction cache, diff between "code" and "data", we won't talk about front-end much
 * Relative costs table bottom pg 16
   + maybe slightly different on a Skylake to a Nehalem processor, but in ball park
   + 80x theoretical difference operating from L1 than memory
     - one reason why we're talking about this! You're only going to get 8x speedup by adding 7 more cores!
   + "Some parts of the cost can be hidden" - *latency hiding*, this is one thing we want to design for! The GPU is dependent on it.
     - How can we start loading a value before we need it?
       * prefetching
       * locality (e.g., on a cache line or memory page that we, or another thread (sharing), have already loaded)
       * can't be done reliably for unpredictable (i.e., randomly branching) code

  * Review his experiments on "working set size" with a linked list
    + Figure 3.9 and code snippet below it
      - sort all the nodes of the linked list before using it (sequential pattern)
      - npad array to control the size of the data (i.e., is one struct instance on one cache line), arbitrary, size that matters
      - varies size of linked list
    + Figure 3.10 (walk seq list)
      - 3 plateaus (L1d, L2, RAM, no L3 on his machine)
      - cost in cycles way less than earlier table
        * prefetching in effect!
        * still a difference though, just not the 80x we anticipated, more like 2x
    + Figure 3.11
      - same plot, but increases padding; i.e., distance between links grows
        * reflects what we saw yesterday about hot vs cold data, much more cold data here
        * actually gets worse than 80x for really large objects; only difference was number of member variables!
    + Figure 3.15 (seq vs random)
      - random 45x worse, just walking the linked list
      - correlated with Figure 3.16
        * exactly what I want to see in the projects! With Table 3.2, concrete evidence to explain the performance trend
          + especially if the data structure design was to optimise for a smaller working set size

  * matrix multiply examples  
    + Figure 6.1/Table 6.1 shows this again for populating a matrix rowwise vs colwise
    + matrix multiplication naive (page 49)
      - clear that one will be iterated colwise: optimisation potential!
      - transposing matrix gives >4x speedup! memory access patterns alone.
      - next = tiling/blocking/unrolling, review pseudocode of 6 nested loops
        * Table 6.2, ca 6x speedup
          + 10x speedup by adding vectorisation. (should be more today, I suspect)
        * careful with tiling in general: compiler should do it
          + can check with godbolt before unrolling
    + key take-away, 10x speedup, just from simd + memory access patterns
      - That's the threshold for a top-level DB paper that proposes a new "algorithm"
        * must look closely: is the new algorithm better, or the implementation? Weak baseline?
