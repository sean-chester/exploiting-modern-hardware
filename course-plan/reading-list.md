# Reading List: CSC 485C/586C (Data Management on Modern Computer Architectures)

The following reading list represents the evolving curriculum for this course. These readings provide context or supplemental resources for understanding the live-coding lectures in which these concepts are applied.

## Background and context
  
  * Sutter (2005). _The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software_. http://www.gotw.ca/publications/concurrency-ddj.htm
  * Sutter (2012). _Welcome to the Jungle: Or, A Heterogeneous Supercomputer in Every Pocket_. https://herbsutter.com/welcome-to-the-jungle/ 


## Optimising a single superscalar core

   * Jacobs (2009). "The Pathologies of Big Data." _Communications of the ACM_ 52(8). https://dl.acm.org/doi/10.1145/1536616.1536632 

   Our first example of experiments showing how memory access patterns and data layout dramatically affect performance. Will be discussed in class on 15 Jan, with a focus on Figure 1 and Figure 3. We will also discuss more broadly the concepts of compute-bound versus memory-bound implementations and the memory wall.

   * Intel (2019). _Intel® 64 and IA-32 Architectures Optimization Reference Manual_, Appendix B.1: "Top-Down Analysis Method." https://software.intel.com/en-us/download/intel-64-and-ia-32-architectures-optimization-reference-manual
   
   Introduces a general methodology for architecture-conscious code profiling for performance. We will in class on 17 Jan use the events listed in Appendix B.2 and learn how to apply this method to isolate performance bottlenecks and drive algorithmic design. This methodology will be used throughout the course to verify algorithmic claims.

   * Drepper (2007). _What Every Programmer Should Know About Memory_, Chapter 3: "CPU Caches" until 3.3 + Chapter 3.3.2 "Measurement of Cache Effects." https://people.freebsd.org/~lstewart/articles/cpumemory.pdf

   Describes cache in terms of how it affects your programming. To be discussed in class 21 Jan. We will see how to reproduce one of the plots.

   * Drepper (2007), Chapter 6.2.1 "Optimizing Level 1 Data Cache Access."

   Uses Chapter 3.3 to optimise matrix optimisations by an order of magnitude. We will discuss and try to reproduce this 22 Jan.

   * Chilimbi et al. (2000). "Making Pointer-Based Data Structures Cache Conscious." _IEEE Computer_ 33(12). http://ezproxy.library.uvic.ca/login?url=https://ieeexplore.ieee.org/document/889095

   Extends Chapter 6.2.1 of Drepper to pointer-based structures, namely search trees. (Matrices, by contrast, are _contiguous_ data structures.) To be discussed 24 Jan.

   * Ghoting et al. (2005). "Cache-conscious Frequent Pattern Mining on a Modern Processor." _VLDB_. http://ezproxy.library.uvic.ca/login?url=https://dl.acm.org/doi/10.5555/1083592.1083660

   High-impact paper with Intel that applies what we've learned by this point to a classic data mining problem (frequent itemset mining). To be analysed 28 Jan.

   * Cohen (2018). "Maximize Cache Performance with this One Weird Trick: An Introduction to Cache-Oblivious Data Structures." Blog Post: https://rcoh.me/posts/cache-oblivious-datastructures/. Accessed: 12 January 2020.

   Blog post that summarises a linked research paper by Demaine (2002). Introduces a theoretical model (cache-oblivious algorithms) that captures the ideas we've investigated until now. May be skipped depending on time.

   * Li & Patel (2013). "BitWeaving: Fast Scans for Main Memory Data Processing." _SIGMOD_. http://pages.cs.wisc.edu/~jignesh/publ/BitWeaving.pdf

   Uses "bit-level parallelism" to optimise cache to achieve "bare metal speed." May be skipped depending on time.


## Exposing and exploiting data-level parallelism for multicore/simd

   * OpenMP Architecture Review Board (2013). _OpenMP Application Program Interface Examples_. https://www.openmp.org/wp-content/uploads/OpenMP4.0.0.Examples.pdf. Accessed 16 Feb 2020.

   Supplemental resource. Provides substantial set of examples to help get started with OpenMP as a c++ parallelisation framework. OpenMP introduced with _parallel reduction_ on Tue 17 Feb 2020.

   * Intel Corporation (2019). "Atomic Operations" In _Intel® Threading Building Blocks Documentation_. https://software.intel.com/en-us/node/506090. Accessed 16 Feb 2020.

   Supplemental resource. Short description of atomic operations. We applied fetch_and_add() in building a lock-free append-only log on Tue 25 Feb 2020.

   * Chester et al. (2015) "Scalable parallelization of skyline computation for multi-core processors." _ICDE_. http://sean-chester.github.io/assets/preprints/chester_icde2015_mcsky.pdf

   Example of updating a shared mutable data structure to improve work-efficiency of a multicore algorithm. Same ideas applied in live-coding to a simplified anomaly detection problem. Wed/Fri 26/28 Feb 2020.

   * Porobic et al. (2012) "OLTP on Hardware Islands." _PVLDB_ 5(11): 1447--1458. http://vldb.org/pvldb/vol5/p1447_danicaporobic_vldb2012.pdf

   Discussion of multicore and multisocket memory model, cache sharing, NUMA effects, superlinear speedup. Tue 3 Mar 2020.

   * Bikker (2017) "Practical SIMD Programming." Technical Report, Universiteit Utretch. http://www.cs.uu.nl/docs/vakken/magr/2017-2018/files/SIMD%20Tutorial.pdf

   Supplemental, instructional resource for live-coding session to introduce SIMD vectorisation. Wed/Fri 4/6 Mar 2020.


## GPGPU and SIMT Parallelism

   * Merrill et al. (2012) "Scalable GPU graph traversal." _ACM SIGPLAN Principles and Practice of Parallel Programming (PPoPP)_, 117–128. http://ezproxy.library.uvic.ca/login?url=https://doi.org/10.1145/2145816.2145832

   Live-coding session in which we construct in parallel a GPU-friendly (compressed row) data structure for graphs (Figure 1 of the reading). Tue 10 Mar 2020. 

   * Bøgh et al. (2016) "SkyAlign: a portable, work-efficient skyline algorithm for multicore and GPU architectures," Sections 2.2 and 4.2. _The VLDB Journal_ 25(6): 817–841. http://ezproxy.library.uvic.ca/login?url=https://dl.acm.org/doi/10.1007/s00778-016-0438-1

   Live-coding session in which we re-derive a GPU-friendly algorithm to construct a flattened octree search structure (Section 4.2 of the reading). Wed 11 Mar 2020. 

   * Lindholm et al. (2008) "NVIDIA Tesla: A Unified Graphics and Computing Architecture." _IEEE Micro_ 28(2): 39–55. http://ezproxy.library.uvic.ca/login?url=https://ieeexplore.ieee.org/document/4523358

   Discussion of GPGPU computing, the NVIDIA GPU architecture, and the SIMT computational model. Fri 13 Mar 2020.

   * Zeller (2011) "CUDA C/C++ Basics". _ACM/IEEE Supercomputing_ Tutorial Slides. https://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf

   Introduction to CUDA. Accompanied with code example converting our [../19-simd/](../19-simd/) lecture into CUDA. Wed 18 Mar 2020.

   * Harris et al. (2007) "Chapter 39. Parallel Prefix Sum (Scan) with CUDA." In _GPU Gems 3_. https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda

   Reviews several types of reductions, including parallel reduction and butterfly reduction, in a massively parallel context. Accompanied by a self-directed lab. Fri 20 Mar 2020.

   * Zhang et al. (2010) "Streamlining GPU applications on the fly: thread divergence elimination through runtime thread-data remapping." _ACM International Conference on Supercomputing_, 115–126. http://ezproxy.library.uvic.ca/login?url=https://dl.acm.org/doi/10.1145/1810085.1810104

   Software techniques for managing branch divergence on the GPU. Accompanied by a self-directed lab that explores _thread-data remapping_. Tue 24 Mar 2020.


## Course Wrap-up

   * Satish et al. (2012) "Can traditional programming bridge the Ninja performance gap for parallel computing applications?" _International Symposium on Computer Architecture (ISCA)_ http://ezproxy.library.uvic.ca/login?url=https://ieeexplore.ieee.org/document/6237038