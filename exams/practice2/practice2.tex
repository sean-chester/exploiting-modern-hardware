\documentclass[addpoints,12pt]{exam}

%\usepackage{newtxtext,newtxmath} % times new roman font
\usepackage{url}
\usepackage[dvipsnames]{xcolor}
\usepackage{enumitem}

\usepackage{bm,mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\usepackage{tikz,pgfplots}
\usetikzlibrary{shapes}

\usepackage{titling}
\setlength{\droptitle}{-6em}

\title{Practice Exam \#2\\[0.25em]
\large CSC 485C/586C: Data Management on Modern Computer Architectures}
\date{}

\begin{document}
\maketitle

\vspace{-6.5em}
{\centering
  \hspace{0.05\textwidth}
  \parbox{0.6\textwidth}{%
    Name:\enspace\hrulefill
  }\hspace{2em}
  \parbox{0.25\textwidth}{%
    V00\enspace\hrulefill
  }
}

\bigskip
\printanswers
\begin{questions}
 \question Please explain the following concepts in 1-3 sentences and/or code snippets and/or a small illustration.  An excellent answer does not have to be long, just precise.\\{\em Guide: 2 minutes each} 

 \medskip
 
 \begin{parts}
   \part[1] Cache Coherence
     \begin{solution}[6em]
The maintenance of consistency between cache lines in separate caches that reference the same underlying memory address.
     \end{solution}
   
   \part[1] Data-level Parallelism
     \begin{solution}[6em]
Parallelisation across elements of data (as opposed to, say, across threads or tasks)
     \end{solution}
   
   \part[1] False Sharing
     \begin{solution}[6em]
When distinct threads reference distinct, independent data elements, but those elements reside on the same cache line. This creates a dependency/contention that logically should not exist.
     \end{solution}
   
   \part[1] Synchronisation
     \begin{solution}[6em]
Causing threads to join up and wait for each other (e.g., in order to coordinate computation or data structures)
     \end{solution}
   
   \part[1] Vectorisation
     \begin{solution}[6em]
The application of vector (i.e., SIMD) registers to accelerate code or the process of exposing data-level parallelism that can be exploited with SIMD.
     \end{solution}
 \end{parts}
 
 \newpage
 \question The plot below shows the relative performance (as measured in execution time) of two parallel algorithms (called "My Algorithm" and "State of the Art") and a sequential baseline ("Seq. Baseline") on a fixed input. It is a {\em parallel scalability plot} that illustrates how performance changes as the number of threads varies, although it shows time rather than speed-up.
Answer the following questions that analyse this figure:\\
{\em Guide: 10 min total}

\begin{figure}[h!]
  \centering
  \input{tikz/work-eff-chart}
\end{figure}

   \begin{parts}
       \part[2] Which parallel algorithm is more work-efficient? Explain how you arrived at this conclusion.
           \begin{solution}[8em]
{\em My Algorithm} is more work-efficient. We can compare the performance of the two algorithms on a single thread, relative to the sequential baseline, and observe that {\em State of the Art} incurs a lot more overhead/additional work.
           \end{solution}

       \part[2] Which algorithm exhibits the best parallel scalability? Justify your response.
           \begin{solution}[8em]
This answer is nuanced. {\em State-of-the-art} gains the most speed-up from additional threads, up to $t=8$. After this point, its performance degrades whereas {\em My Algorithm} continues to see (diminishing) returns. This suggests that either we have 8 physical cores and the last 8 are logical cores/hyperthreads or that the final 8 cores are on a separate socket and {\em My Algorithm} has better NUMA performance than {\em State of the Art}.
           \end{solution}

       \part[2] What is the overall message (a.k.a., purpose, or significance) of the plot?
           \begin{solution}[8em]
This plot shows performance relative to thread count. it emphasises raw execution time, but parallel scalability and work-efficiency can both be read from it. The primary message is that {\em My Algorithm} is the fastest or near-fastest algorithm on systems with at least 4 cores, but that it takes 4 cores to amortise the overhead for both parallel algorithms.
           \end{solution}
   \end{parts}
 
 
  \question[8] Describe how you would vectorise/SIMD'ise the code below. Pseudocode or annotations is fine; proper syntax is not evaluated, so long as the intent is clear.\\
{\em Guide: 15 min}
  
   \bigskip
    \begin{minipage}{.35\textwidth}
      \begin{verbatim}
struct point
{
  double x;
  double y;
  double z;
};

std::vector< point > points;
      \end{verbatim}
    \end{minipage}
    \hfill
    \begin{minipage}{.55\textwidth}
      \begin{verbatim}
template < typename T >
auto num_matches( T const& points)
{
    auto const n = cards.size();
    auto num_matches = 0llu;

    for( auto i = 0u; i < n; ++i )
    {
        for( auto j = i + 1; j < n; ++j )
        {
            if( points[ i ].x == points[ j ].x
             && points[ i ].y == points[ j ].y )
                ++num_matches;
        }
    }
    return num_matches;
};
      \end{verbatim}
    \end{minipage}
    

      \begin{solution}[25em]
To take full advantage of the SIMD-width, we need to rewrite the vector of points as three separate aligned vectors, one for each coordinate. Then we can rewrite the loops to compare multiple $x$'s to each other, multiple $y$'s to each other, and multiple $z$'s to each other for equality. Thereafter, we can take the bitwise conjunction of the three result mask vectors and then count the number of set bits. Bonus mark for handling alignment around the edge cases of $j$ correctly. E.g.:

      \begin{verbatim}
std::vector< double, MyAlloc > xvals;
std::vector< double, MyAlloc > yvals;
std::vector< double, MyAlloc > zvals;


template < typename T >
__mm256 num_matches( T const& points)
{
    auto const n = cards.size();
    auto matches = 0llu;

    for( auto i = 0u; i < n; i+=8 )
    {
        __m256d const xi = _mm256_load_pd( &( xvals.data() + i ) );
        __m256d const yi = _mm256_load_pd( &( yvals.data() + i ) );
        __m256d const zi = _mm256_load_pd( &( zvals.data() + i ) );

        for( auto j = i + 1; j < n; j+=8 )
        {
            __m256d const result = _mm256_and_pd(
                _mm256_cmp_pd(
                    xi, _mm256_load_pd( &( xvals.data() + j ), 0)
                ),
                _mm256_and_pd(
                    _mm256_cmp_pd(
                        yi, _mm256_load_pd( &( yvals.data() + j ), 0)
                    ),
                    _mm256_cmp_pd(
                        zi, _mm256_load_pd( &( zvals.data() + j ), 0)
                    )
                )
            );
           matches += __builtin_popcount(_mm256_movemask_pd( result ));
        }
    }
    return matches;
};
      \end{verbatim}
      \end{solution}
    
    \newpage
    \question You are given a set of randomly located unit (i.e., of size $1\times1$) squares in the Euclidean plane and are to determine how many of them intersect each other. Please write a work-efficient algorithm that scales well with parallelism to solve this task. It should do less work than the naive quadratic solution that compares every point to every other point.

Your best strategy will be to create a two-step algorithm. In the first step, you should preprocess the data. In the second step you should operate on the preprocessed data to solve the problem efficiently.

The focus of this question is on how you expose parallelism. You are free to describe the algorithm however you want (e.g., pseudocode, c++, a mixture thereof), but it should be specific enough that it is clear what are the distinct parallel tasks and what synchronisation and/or contention occurs.\\
{\em Guide: 20 min}

      \begin{solution}[8em]
This is similar to the example in class for finding isolated points. There are many possible solutions. One of the simplest is to first sort the data based on L1/Manhattan Norm. In the second step, we still use a double-nested loop, but no point $i$ needs to be compared to any point $j$ that has a Manhattan Norm more than 2 units larger than that of $i$.
      \end{solution}
\end{questions}

\vfill
\begin{minipage}{.2\textwidth}\hphantom{xxxxxxxxxxx}\end{minipage}
\gradetable[h][questions]

\end{document}
